\documentclass[12pt,a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{mathtools, qtree}
\usepackage{amssymb,amsmath, fancyhdr}
\usepackage{tikz}
\usetikzlibrary{automata,positioning}
\usepackage{listings}
\usepackage{graphicx}
\pagestyle{fancy}
\title{Term Project: Q-Learning}

\date{07 June 2017}
\author{Cody Shepherd}

\lhead{Shepherd}
\rhead{Q-Learning}

\lstset{
    keywordstyle=\color{blue}
  , basicstyle=\ttfamily
  , commentstyle={}
  , columns=flexible
  , numbers=left
  , showstringspaces=false
  }

\begin{document}
\linespread{1.25}
\maketitle

\normalsize

	\section{Introduction}
	
	\par The goal of my project was to implement and evaluate a working Q-Learning algorithm using Haskell. Q-Learning is a reinforcement algorithm that uses rewards to learn a mapping from state-action pairs to values. In the case of my project, the Q-Learning problem was framed in terms of an agent, or ``robot", moving around a grid picking up objects. The goal of the agent was to learn a mapping that would allow it to pick up the most objects during testing.
	\par One of the major components of this project involved improving on an earlier attempt at implementing the Q-Learning algorithm in Haskell. My earlier attempt attempted to use recursion to perform repeated actions during training and testing, and had problems with stack overflows. My implementation for this project made use of monads and the Kleisli composition operator instead of attempting explicit recursion, and was met with success.
	\par The other interesting aspect of my project involves a performance comparison between a (correct) implementation of the algorithm I wrote in Python, and the current Haskell version. Time-based testing indicates that the Haskell version outperforms the Python version in terms of execution time.
	
	
	\section{Related Work}
	
	\par The concept of this project came out of the Machine Learning course taught by Professor Melanie Mitchell. I took that course in winter quarter 2017, and my first attempt at the Q-Learning algorithm I wrote in Haskell (which I had partially learned the quarter before that in CS 320). Because of problems with that initial implementation, I ended up re-writing it in Python in order to have a working program to turn in. 
	
		\subsection{Q-Learning}\label{beefcake}
		
		\par Q-Learning is a reinforcement algorithm that trains an agent to learn to perform tasks in an environment \cite{RL}. In contrast to a static policy (e.g. state machine), which maps states to actions, the q-learning algorithm is concerned with approximating an optimal policy by mapping state-action pairs $(s, a)$ to real values. At a given timestep, the agent only has access to its current observable state together with a list of available actions from the given state. It must learn the best policy $\pi : S \rightarrow A$ to maximize some reward function $\delta : S \times A \rightarrow \mathbf{R}$.
		\par The strategy taken by Q-Learning is to approximate $\pi$ via a Q-function $Q(s,a)$, which not only maps state-action pairs to values, but generates those values from the assumption that the agent will follow the best policy into the future.
		\par The Q-function is formally expressed by $Q(s,a) = r(s,a) + \gamma \text{max}_{a^\prime} [Q(\delta(s,a), a^\prime)]$.
		\par Here, $r(s,a)$ represents the reward acquired from taking action $a$ at state $s$, $\text{max}_{a^\prime}$ represents the reward taken from the best possible action $a^\prime$ at the state $s^\prime$ produced by action $a$, and $\gamma$ represents a discount factor that allows the system to value future rewards lower than immediate rewards. The entire expression amounts to ``taking action $a$ at state $s$ gives some reward plus the discounted reward from the best future action." This gives the algorithm the ability to ``think ahead" and consider the long-term as well as the short term. We also include a learning rate, $\eta$, applied to the right side of the equation, when training. $\eta$ is the ``learning rate," i.e. the significance to which the algorithm gives any one ``lesson."
		\par In practice, the Q-function can be represented in several ways. Scientists and engineers at Google used a deep neural network for learning and representing the Q-function in their AlphaGo player \cite{alphago}. For all of the programs involved in this project, I have used a Q Table (also called a Q Matrix). One might think of the Q Table as 2-D array of reward values (Floats or Doubles), where the row indices are governed by a given state -- the five observable cells from a given grid position (north, south, east, west, and ``here"; diagonals are ignored) -- and the column indices governed by the five actions possible from an arbitrary state (move up, down, left, right, or attempt a pickup). In reality, I implemented the Q Table, in both the Haskell and Python versions, using an associative Map, but the array analogy is perhaps in nicer agreement with the theoretical details of Q-Learning.
		\par The Q-Learning algorithm, and indeed much of machine learning, is ultimately divided into two major phases: training and testing. The algorithm learns the Q-function during training via the following algorithm \cite{RL}:
		
		\begin{itemize}
			\item Observe state $s$.
			\item Select action $a$.
			\item Take action $a$ and receive reward $r$.
			\item Observe new state $s^\prime$.
			\item Update the Q Table; $Q(s,a) \Leftarrow Q(s,a) + \eta(r + \gamma \text{max}_{a^\prime}(Q(s^\prime, a^\prime)) - Q(s,a))$.
			\item Update $s \Leftarrow s^\prime$.
		\end{itemize}
		
		\par Action selection is governed by an ``$\epsilon$ -greedy" strategy, which seeks to strike a balance between exploration and exploitation. At the start of training, we initialize the parameter $\epsilon$ to 1. During action selection, the algorithm selects an action based on the probability of $\epsilon$. Therefore, initially every action is random -- exploration is maximized. The $\epsilon$ parameter is then ``annealed" over the training episodes: that is, every 50 episodes it is decremented by 0.01 until its value is 0.1. This means that at the other end, every 10th action (on average) taken by the fully trained algorithm will be random -- exploitation of the learned policy is favored over exploration, but exploration is never fully eliminated. As we will see below, this can sometimes allow the algorithm to break out of a bad policy. 
		\par Training is divided into steps and episodes. A single loop through the above algorithm represents a step; an episode consists of 200 steps, starting from a random location in a randomly-initialized board. During training, the algorithm runs through 5,000 episodes; while the boards are randomly reinitialized every episode, the Q Table is maintained from episode to episode, and ultimately passed out at the end of training for use during testing.
		\par For the purposes of this project, all starting boards have objects (``cans") in approximately 50\% of cells in an 8x8 board. Technically, the boards are all 10x10, with ``walls" on the outermost layers.
		\par During training, we gather metrics on the Q Table after each episode. Specifically, we are interested in the sum total reward stored in the Q Table after each episode. This allows us to visualize and understand the rate at which the algorithm learns, and its behavior during its attempt to maximize reward.
		\par The goal of testing is to acquire metrics used to evaluate the performance of the trained policy. The algorithm does not update its Q Table during testing; once training is done, the Q Table is static, although it still is passed along from testing episode to testing episode. Testing also runs 5,000 episodes of 200 steps each, randomly reinitializing the board state and agent location at the beginning of each episode. Testing is primarily concerned with reward acquired during each episode. There are many different ways to acquire and sift this and other types of data; for the purposes of this project, we collect the sum of rewards received for each episode during testing, then compute the average over the sums at the very end to evaluate the learned model.
		\par For a project of larger scope, it would have been interesting to instrument and compute statistics such as precision and recall, F-Score, or harmonic mean of rewards. Average reward over episodes only gives a very general basis for comparison between two models. Further metrics would allow us a more fine-grained understanding of where and why models performed as they did.
		\par In all implementations of training discussed in this project, the algorithm received a reward of 10 for a successful pickup of a can, a -5 penalty for colliding with a wall, a -1 penalty for attempting a pickup where there was no can, and no reward otherwise.
	
		\subsection{First Attempt}
		
		\par The initial design for my program was in some ways similar to the design for this project. I created alternative types for grid cells and for actions:\\
		
		\begin{lstlisting}[language=Haskell,numbers=none]
data Cell = Empty | Wall | Can | ERob | CRob deriving (Show, Eq)

data Act = U | D | L | R | P deriving (Show, Eq)
		\end{lstlisting}\ \\
		
		 \noindent Not unlike the implementation of the Dir type in my current program:\\
		
		\begin{lstlisting}[language=Haskell,numbers=none]
data Dir =  U
	 | D
	 | R
	 | L
	 | P
	 deriving (Show, Eq, Enum, Ord)
		\end{lstlisting}\ \\
		
		\par During my first attempt, I tried to store the grid and a given robot state as heavily parameterized types. In retrospect, I was trying to force an imperative ``struct" style that ended up being very awkward to handle:\\
		\begin{lstlisting}[language=Haskell,numbers=none]
data Grid = Grid{ cells :: [[Cell]]
	,   loc     :: (Int, Int)
	,   reward  :: Float
	,   step    :: Int
	,   qtable  :: Map String [Float]} deriving (Show, Eq)

data State = State{ north :: Cell
                  , south :: Cell
                  , east  :: Cell
                  , west  :: Cell
                  , here  :: Cell
                  , key   :: String} deriving (Show, Eq)

		\end{lstlisting}\ \\
		
		\par The other major mistake in my old program was my attempt to use explicit recursion to perform looping over steps and episodes. I knew that a traditional use of the stack to do this would not be sufficient, that it would blow up at some point, but I was also aware of Haskell's capability to optimize use of the stack out of functions utilizing tail recursion -- my first program was based around the gamble over whether or not I could achieve tail recursion, and/or use it effectively.
		\par Here we have an example from my first program that attempts to execute many training episodes using this explicit recursion:
		
		\begin{lstlisting}[language=Haskell,numbers=none]
		
execEpisode :: [Act] -> [Bool] -> Int -> Grid -> Grid
execEpisode rands isRands i grd = let s = getState grd
	act = getNewAction rands isRands grd
	grd' = move act grd
	s' = getState grd'
		in if i == m_global
			then learn s s' act grd'
			else execEpisode rands isRands (i+1) 
				(learn s s' act grd')

		\end{lstlisting}\ \\
		
		\par Part of the problem here (I think) is lazy evaluation of all the parameters being passed into the recursive call. It may be that forcing these evaluations prior to entering/evaluating the next call would have obviated some of the disaster that I experienced (in the form of stack overflow). It could also be that this function is not truly tail recursive, although I cannot see where. Furthermore, this is not the only function in the program designed this way, so it may not be the culprit. 
		\par My lack of understanding of IO and Monads in general prevented me from wanting to spend a lot of time working up tests last quarter, and I decided that the best use of my time would instead be to implement the program in a language I already knew.
		
		\subsection{Python Version}
		
		\par The working program that I eventually wrote in Python took an object-oriented approach. It implemented a class each for a grid cell, the grid/board itself, the Q Table, and the algorithm manager. A cell maintained its status, which of course could be updated in an imperative manner. The board object instantiated the grid explicitly as a 2-D array of cells and maintained state used for bookeeping and tracking of objects on the grid. The Q Table object acted as a domain-specific interface to a Python dict where the pair-value mappings were kept. And the algorithm manager held state and implemented the methods necessary for carrying out the algorithm defined in section \ref{beefcake}.
	
	\section{Project Design \& Implementation}
	
		\subsection{Board Representation}
		
		\par My current program diverges from both the previous Haskell attempt and the working Python program in important ways, but it also borrows a fair amount from both prior versions.
		\par Instead of explicitly representing the board environment as the grid defined by the robot problem description, I chose to track the can locations (as a list of Can types), the robot's location (as a single Rob type), and the dimensions of the grid (as a tuple of Ints) separately. Whether this saves space seems debatable. What it allowed me to do was throw away the explicit Cell object from prior versions, and depending on language implementation and internal object state, the amount of memory required for a full 2-D array or list could be non-trivial. 
		\par Performance-wise this also seems like a debatable trade-off. One advantage to not having Cell objects is not incurring the time required for instantiating each of these memory objects. However, in an imperative program, this instantiation need happen only once per episode; so saving on a constant-time operation is probably not that big of a get. We also lose the constant-time array access when observing state or checking moves. However, for a grid of small size, because we know that only around half of the cells will have Cans on them, searching cans is a relatively trivial operation -- although it is technically linear WRT board size, with the board size used in this project it might as well be constant time. 
		\par An additional advantage involves the functional paradigm: if we were to track every cell in the grid, we would be required to regularly rebuild the grid as cans were picked up. Because the functional paradigm prevents us from simply updating the state of a cell, heavy list processing would be involved in swapping a cell containing a can out for an empty cell. Likewise for whenever the robot moved from one cell to another. Instead, with the new implementation, when the robot moves we can simply replace the Robot parameter in the board -- a constant time operation -- or, when a can is picked up, cut an item out of the cans list -- a linear time operation for which the language is well-suited.
		\par Although for an imperative language, a 2-D array seems like a great way to go, in this case it appears to be a comparison between ``apples and oranges." Design choices appropriate for one do not necessarily carry over to the other.
			\par Imperative to the training and testing algorithm described in section \ref{beefcake} are randomized board states and robot locations. This was one of the major challenges during my first attempt at writing this program in Haskell -- it had not occurred to me that random number generation would violate the assumptions of purity of some standard function. IO and Monads in general were poorly-understood concepts to me at the time, and I believe not having a solid understanding of them is what ultimately prevented me from getting the program working the first time.
		\par My new design choices about board representation also helped me here, however. Instead of having to decide for each cell whether it should be empty or have a can on it, or whether the robot should start there, I could instead simply decide what percentage of the total cells should have cans, and generate a list of cans by generating random pairs within some constrained range. This ended up allowing me to isolate IO behavior much more easily.

		\subsection{Learning}
		
		\par For observing its state, the agent needs a way of differentiating the condition of one state from that of another. For this, my solution was very similar to both prior versions: having some notion of the contents of a cell, described by an alternate type (the State type in the code). Furthermore, for quick storage and retrieval of a given state in the Q Table, we need some way of generating a unique "key" from an observation. My choice for this also followed previous versions: a length-5 text string, generated programmatically, representing each possible observable state.
		\par Action selection was governed by the $\epsilon$-greedy policy -- first generating a random number to assert randomness or informed selection, and then selecting an action based on the results. Selecting a random action was accomplished by yet another random number generation. Selecting the best action required a lookup of the current state from the Q Table and determining which action ``column" in the table held the highest value. 
		\par A training step in my program simply transforms one Q Table, Board pair into another via a move action and its resulting reward computation. Repetition of this process over 1 million steps (5,000 episodes of 200 steps each) incrementally fills out the Q Table and models a ``best" heuristic for the agent's behavior on the grid.
		\par The function updating the Q Table was the one most prone to error. It required ``unpacking" two Maybe types from Map lookups, and constructing new lists of values from the Q function to replace the original lookup value. Handling the possibility of ``bad lookups" perhaps should not have been strictly necessary, given a sufficiently correct Q Table build algorithm, but seemed prudent nevertheless.
		\par The Q Table build function -- \texttt{newQTable} -- built a blank Q Table by filling an associative map with strings representing all possible combinations of state values. This process produced state keys that would never actually be seen in practice, such as state strings containing three or more walls, but the amount of superfluous memory wasted by these results was outweighed by the ease of writing and verifying the process.
		\par Executing a test step ended up being very similar to executing a training step, with the major difference being treatment of the Q Table. The Q Table is updated each training step, but remains static throughout testing. The function \texttt{test} still takes and produces a QTable, Board, Double triple, however, because it must be able alter the \textit{board} and pass everything on to the next test step when composed with the Kleisli operator. The Double value taken and produced by the \texttt{test} function represents the cumulative reward obtained throughout a test episode. 
		
		\subsection{Executing the Algorithm}
		
		\par Having training and test steps working is all well and good, but ultimately not enough. This was the point at which I became stuck during my first attempt at a Haskell program. Specifically, the challenge was how best to accumulate or capture the updated board and Q Table from step to step, passing the output of one step into the next in an efficient way. I was eager to stay away from explicit recursion, as it had landed me in a poor situation during my first attempt.
		\par The answer ended up being the Kleisli Composition operator -- \texttt{(>=>)} -- from the Control.Monad library \cite{kleisli}. \\
		
		\begin{lstlisting}[language=Haskell,numbers=none]
(>=>) :: Monad m => (a -> m b) -> (b -> m c) -> a -> m c
(m >=> n) x = do
	y <- m x
	n y
		\end{lstlisting}\ \\
		
		This operator takes the monadic output from one function and feeds it into the next as a non-monad, returning a function from the input of the first to the output of the second. The behavior of this operator is associative.
		\par It turned out that this was exactly the operator I needed to get my program working. My episode functions made use of it by first generating lists of \texttt{train} or \texttt{test} functions, partially applied to their configuration/control arguments, and then folding the Kleisli operator over the list, beginning with an initial board and Q Table pair. It was then easy to produce many episodes by repeating this process from a \texttt{doTraining} or \texttt{doTesting} wrapper function. 
		\par This was the breakthrough I needed! Thanks to Professor Jones for helping me out with this one.
		
		\subsection{Instrumentation}\label{inst}
		
		\par An additional problem presented itself in the form of an instrumentation problem. How could I exflitrate the metrics that I needed to test and observe my algorithm? What I needed, at the very least, was the total reward content stored in the Q Table after each training episode, as well as the total reward obtained by the agent during each testing episode. The first metric would give me an idea of how well the algorithm was ``learning." By watching the total contents of the Q Table rise or fall, I could tell if the algorithm was making a good distribution of moves, and if its efficiency was improving over training episodes. The second metric would give me an idea of how well the \textit{trained} algorithm performed overall during testing.
		\par The solution to this problem ended up being a matter of passing reward values through the test and train functions, and in turn the episode functions, along with the board and Q Table states. 
		\par I am not totally satisfied with this solution, however, as it is very inflexible on account of being ``hard-coded" into the algorithm. It would be very tedious to add further instrumentation to collect metrics in this way.
		\par What could be a better possible solution? Maybe it would involve writing to a file during each step? This seems like it would be far too slow. Maybe create a parameterized type to hold several different types of values, and include some accompanying function for filling the type? This seems the likely answer; although it would still require hand-coded adjustments when new instrumentation needed to be added, at least these changes would only need to take place in just one place. 
		\par However, my worry with this solution would be the memory management requirements involved in building a new, complex parameterized type for each step. Could we simply introspect the functions as they execute from the outside, perhaps from a parallel thread? I still don't have a great answer to this question.
		\par In the end, the instrumentation that made it into this project represents ``minimum viable product" in that it afforded me the bare minimum of metrics required to have a basic assessment of the algorithm, but provides very little in the way of convenience beyond that. 
		
			
	\section{Testing \& Results}\label{testingresults}
	
	\par In this section I'll discuss my process of testing my program, comparing it to a Python version, and collecting and interpreting the data it generated.
	
		\subsection{Testing}
		
			\subsubsection{Correctness}
			
			\par Testing for this program was done through manual inspection of outputs. When testing, I checked output values against expected values, and also used output values in assessment of ``expected behavior" of the algorithm in the context of my previously written working program.
			\par Of utmost importance was board generation, and agreement between the formal board parameters and the visual representation of the board. We can start small and manually check these values: \\
			\begin{lstlisting}[language=Haskell,numbers=none]
*Qlearn> b <- randBoard (4,4) 0.5
*Qlearn> b
Board ((4,4),[Can (2,1),Can (3,2),Can (1,2),Can (2,0),Can (3,1),Can (3,3),
Can (1,3)],Rob (0,0))
*Qlearn> showBoard b
[ ][.][.][.]
[.][.][ ][ ]
[ ][ ][.][.]
[o][ ][ ][ ]
			\end{lstlisting}\ \\
			
			\par Here we expect that the all the Can and Rob coordinates lie inside the board dimensions, i.e. $0 \leq i < dim$. We also expect there to be a number of cans approximately equal to the floating point value passed as the second argument to \texttt{randBoard}, in this case $0.5$. I use the word ``approximately" here because of two reasons: 1. integer values means an exact percentage may not be possible, and 2. the cans are randomly generated and duplicates may occur (in which case they are pruned out by the algorithm).
			\par Furthermore, we expect the display generated by \texttt{showBoard} to agree with the formal contents of the Board. Hand-verifying in small cases like these is relatively easy, but also slow and based on my own perceptive abilities. Given more time, automated testing could have been implemented to attain wider coverage.
			\par Keep in mind that the first value in a tuple represents its $i$ value, or location on the $y$-axis, and the second value represents its $j$ value, or location on the $x$-axis. This is slightly clunky from an array manipulation, computer science perspective, but fits nicely into the visual implementation of a grid as a list of lists. It also agrees with mathematical treatments of matrices.
			\par We can scale this up; the manual examination becomes more tedious but at least we have some assurance that the program is behaving as expected:\\
			
			\begin{lstlisting}[language=Haskell,numbers=none]
*Qlearn> b <- randBoard (8,8) 0.2
*Qlearn> b
Board ((8,8),[Can (1,4),Can (7,0),Can (2,0),Can (0,0),Can (1,5),Can (0,2),
Can (4,6),Can (6,2),Can (7,2),Can (4,0),Can (2,2),Can (0,3)],Rob (0,6))
*Qlearn> showBoard b
[.][ ][.][ ][ ][ ][ ][ ]
[ ][ ][.][ ][ ][ ][ ][ ]
[ ][ ][ ][ ][ ][ ][ ][ ]
[.][ ][ ][ ][ ][ ][.][ ]
[ ][ ][ ][ ][ ][ ][ ][ ]
[.][ ][.][ ][ ][ ][ ][ ]
[ ][ ][ ][ ][.][.][ ][ ]
[.][ ][.][.][ ][ ][o][ ]
			\end{lstlisting}\ \\
			
			\par Here we have 12 cans ($12.8$ being the exact figure for $(8 \times 8) \times 0.2$), and we can see that all coordinates fall within the given dimensions.
			\par We are going to ignore some edge cases here, such as board dimensions $(0,0)$, because such a case would be outside the problem domain -- i.e. it would make no sense and be of no use to train a Q-Learning agent on a board with no squares. 
			\par Additionally, I was able to use the \texttt{showBoard} function to verify that the agent's movements on the board were happening as expected. I won't reproduce the output here for space considerations, but one can see in the code appendix where the print statements facilitating this kind of inspection have been commented out. This is another feature I would have liked to have had the ability to toggle on and off from a terminal, but again, this feature was cut out due to time constraints.
			\par Testing correctness of the Q Table update function was achieved through gathering and plotting the metrics described in section \ref{inst}. Because it is very difficult to look at the printout of a Q Table -- all of its keys and values enumerated in sequence -- and be able to tell correctness from incorrectness, it was necessary instead to perform a more qualitative kind of verification in this case. Results from this type of testing are shown and discussed later on in section \ref{testingresults}.
			\par Automated testing for this program ended up being an insurmountable obstacle, given the scope and time constraints of the project. 
			\par I made a foray into writing properties that would be checkable with Quick Check, and was led toward Monadic checks (e.g. \texttt{forAllM}) because of the nature of my program. My goal was to write checks along the semantic lines of ``the cans and agent of any randomly-generated board should always fall inside its dimension values," and maybe ``the updated value in any Q Table should be equivalent to the Q function applied to the original value." 
			\par The barrier to the first type of check was my lack of understanding: the Quick Check examples provided in class did not address testing IO functions that I could tell, and the literature online was both wide and deep. 
			\par The barrier to the second type of check was limited time and scope, from an engineering standpoint. Writing such tests manually, without the assistance of an automated tool such as Quick Check, would have taken a lot of time that I didn't have and would have been at least as error prone as the functions they were meant to test.
			
			\subsubsection{Behavior \& Performance}
			
			\par I was interested in comparing my new program with my previously written working program, written in Python. I knew that the Python version was correct (or at least sufficiently correct to receive an A in the Machine Learning course), so a comparison between the data gathered from both programs would help me get an idea of whether my new program was correct as well. Additionally, I was curious to see how the two programs compared in terms of time required to execute the same training/testing sequence.
			\par Both programs report their results by generating a graphical plot in the form of a pdf or png. This would be the way in which I would gather information about the behavior of each program. See these results and discussion in sections \ref{data} and \ref{interp}.
			\par For comparing performance, I conducted some tests by running each program from a bash shell composed with a call to the bash utility \texttt{time}. The programs were both run on the same 2016 Macbook with a dual core Intel m3 processor and 8 GB of RAM. The Haskell program was first compiled using GHC. Both programs train an agent on a 10x10 board (where walls make up the outermost cells) for 5,000 episodes of 200 steps, then test the trained agent for 5,000 episodes of 200 steps. Here are the initial results:\\
			
			\begin{lstlisting}[numbers=none]
Codys-MacBook:qlearning cody$ time ./main
Training...
Training finished.
Testing...
Testing finished.

real	0m48.034s
user	0m47.207s
sys	0m0.701s
Codys-MacBook:qlearning cody$

Codys-MacBook:qlearn cody$ time python qlearn.py

real	1m37.250s
user	1m36.285s
sys	0m0.739s
Codys-MacBook:qlearn cody$
			\end{lstlisting}\ \\
			
			\par It would appear that the Haskell program is much faster than the Python. However, we have a wildcard in this comparison: namely, the library calls performing the image generation. Maybe the library calls are skewing the comparison? This seemed to warrant running both programs again without the image generation step at the very end:\\
			
			\begin{lstlisting}[numbers=none]

Codys-MacBook:qlearning cody$ time ./main
Training...
Training finished.
Testing...
Testing finished.

real	0m43.698s
user	0m42.876s
sys	0m0.608s
Codys-MacBook:qlearning cody$

Codys-MacBook:qlearn cody$ time python qlearn.py

real	1m39.152s
user	1m38.211s
sys	0m0.715s
Codys-MacBook:qlearn cody$
					\end{lstlisting}\ \\
					
					\par This seems to have not made any difference. 
					\par Maybe it comes as no surprise that a compiled program runs faster than an interpreted one. And it could very well be that some of the design choices I made in the Python version were not the best. I've never had formal instruction on Python, after all. And maybe comparing the performance of the two languages in this way tells us very little, since the programs were not written using the same abstractions or algorithms. Still, the results are interesting and encouraging -- using Haskell has paid off in terms of performance, in this case!

		\subsection{Data Collected}\label{data}
		
			\par As discussed above, the primary metric of concern when running this program was the rewards accumulated over all training episodes, and the average rewards obtained from each testing episode. These were then plotted on a graph. Results varied a bit from experiment to experiment, but here is a typical example:\\
			
			\includegraphics[width=\textwidth]{rewards_eps0_1}
			
			
		\subsection{Interpretation of Data}\label{interp}
		
		\par At the top of the image, the average reward acquired over all testing episodes is listed. This gives us some idea of how well the agent did in a given episode during testing -- in this case we might say that it was able to pick up around 12 of the 30-ish cans on the randomized board of any given episode. This might not be totally accurate, as it does not account for ``mistakes" such as attempting pickups without cans being present, or attempting to move into walls, but it gives us some idea.
		\par The plotted line shows the sum of all values stored in the Q Table at the end of a given training episode. In some ways this gives us an idea of when most of the learning takes place during training -- we see that the algorithm learns most of the particulars of its policy by the 1,000th episode, and merely finetunes thereafter. 
			\par From this information, we can infer that this very basic Q-Learning algorithm is working correctly. However, these metrics give us only a basic idea of what is going on with the algorithm. Truly understanding the model generated or derived by a machine learning algorithm remains an area of open research \cite{nguyen}.
			
	\section{Reflections}
	
		\subsection{Major Challenges}
		
		\par The biggest hurdle to getting this program working, from the perspective of my first attempt, was having a good understanding of IO with Haskell and Monads in general. The solution to that hurdle, of course, was to take the Functional Languages class, and learn about how Monads are themselves like programs that produce outputs, or potentially have side effects. 
		\par Additionally, it was my goal to design the program in a way appropriate to the functional paradigm. I don't think that I accomplished this during my first attempt: using heavily parametrized types as stand-ins for struct-like objects and attempting to ``loop" using explicit recursion were my two big failures here. They made my job as a functional programmer more difficult, and in some ways occluded the way toward a functionally-appropriate answer. Redesigning the program with a better understanding of Haskell helped me to keep the functional idioms in mind and ultimately to have an easier time of finding solutions to the small hangups I encountered.
		\par Finally, the ``hump," as it were, was finding a way to pass the output of one Monadic function as non-monadic input to another, accomplished using the Kleisli operator. Thanks to Professor Jones for his guidance here! After this problem was solved, writing the rest of the program was easy.
		
		\subsection{Haskell vs. Python}
		
		\par I don't believe that my ``performance comparison" between the two programs represents definitive proof that Haskell is faster than Python, at least any more than do the observations that the Haskell program was compiled, and the language is typesafe, whereas the Python program is interpreted, and Python performs lots of type conversions at runtime. 
		\par To acquire a true ``apples to apples" comparison would require that the two programs be written in as close to the same way as possible, using the same abstractions and algorithms and design patterns. As this may not always be possible, care would be required when choosing language-specific alternatives. 
		\par Furthermore, the execution environment would need to be more controlled. With a two-core processor running lots of other applications simultaneously, there could have been interference at the hardware or kernel level in the experiments I conducted. Granted, the times I observed are reproducible on my machine and do not show a lot of variance, but from a scientific standpoint the execution environment is still a variable that requires controls. 
		\par The big takeaway from the comparison for me was an assurance that, even with all the linear-complexity list processing tasks such as folds, maps, filters, and finds, the Haskell program performs at least as well, in an asymptotic sense, as a program that utilizes O(1) array access and imperative-style side effects. I think that this kind of assurance is essential to have for any language one uses in practice, and being able to do this kind hands-on experimentation is a great way to reinforce that assurance.
		
		\subsection{Quality of Solution \& Improvements Over First Attempt}
		
		\par Is my solution the best possible implementation of the Q-Learning algorithm in Haskell? Almost certainly not. It seems likely that there is a more efficient way to represent the board and its pieces, such as perhaps a bitboard (where each grid square is represented by a bit, meaning the entire board can fit inside a single word or double-word). It might also be worth it to find or write a data structure for the Q Table whose lookup routine does not return a Maybe type that requires unpacking -- if we were to build such a data structure with assurances of complete state coverage, we would not need to handle the possibility of null returns. Probably the algorithm for observing the current state could be optimized to perform only a single pass over the cans list, rather than five separate passes for each relevant cell.
		\par Additionally, to make such a program truly useful for thorough experimentation, it would be nice to have some kind of terminal for parameterizing, dispatching, and gathering data from experiments. Exploration of the algorithm becomes interesting when one starts to fiddle with the learning rate, the epsilon value, the different rewards for each move, and the number of steps and episodes executed. Changing all these parameters manually in the code base is obviously not the way to go, but building a terminal for this program would require further thinking about function parameters, possibly global values, and a major redesign with these types of adjustments in mind. 
		\par Ultimately this program represents a major improvement over my first attempt. This might be obvious, because this program works and the other one did not. However, I think the major win here is that the newer program has been written with the functional paradigm in mind, and shows evidence (at least to me) of a more mature understanding of the particulars of Haskell and functional programming in general. My attempts in writing the first program to bring some imperative abstractions into the functional world were likely the true barriers to success in that endeavor. Writing this program successfully has provided a great example of how to adapt one's thinking, ``switch gears" so to speak, and consider the implementation of a computing problem from more than one angle. I find that I now have a new set of tools with which to consider a problem and build an appropriate solution. This feels like a major step in my development as a computer scientist.

	\pagebreak
	\begin{thebibliography}{9}
		\bibitem{RL}
		Melanie  Mitchell.
		\textit{Reinforcement Learning}.
		http://web.cecs.pdx.edu/~mm/MachineLearningWinter2017/RL.pdf
		\bibitem{kleisli}
		Monad laws.
		https://wiki.haskell.org/Monad\_laws
		\bibitem{nguyen}
		Nguyen A, Yosinski J, Clune J. 
		\textit{Deep Neural Networks are Easily Fooled: High Confidence Predictions
for Unrecognizable Images.}
		In Computer Vision and Pattern Recognition (CVPR ?15), IEEE, 2015.
		\bibitem{alphago}
		Silver, D., A. Huang, Aja, C. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, D. Hassabis. 2016. \textit{Mastering the Game of Go with Deep Neural Networks and Tree Search}. Nature 529 (28 Jan): 484--503.
	\end{thebibliography}
	
	\appendix
	\section{Github}
	
	\section{Code}
	
	
            
            
\end{document}