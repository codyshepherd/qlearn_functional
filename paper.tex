\documentclass[12pt,a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{mathtools, qtree}
\usepackage{amssymb,amsmath, fancyhdr}
\usepackage{tikz}
\usetikzlibrary{automata,positioning}
\usepackage{listings}
\usepackage{pdfpages}
\usepackage{graphicx}
\pagestyle{fancy}
\title{Term Project: Q-Learning}

\date{07 June 2017}
\author{Cody Shepherd}

\lhead{Shepherd}
\rhead{Q-Learning}

\lstset{
    keywordstyle=\color{blue}
  , basicstyle=\ttfamily
  , commentstyle={}
  , columns=flexible
  , numbers=left
  , showstringspaces=false
  }

\begin{document}
\linespread{1.25}
\maketitle

\normalsize

	\section{Introduction}
	
	\par The goal of my project was to implement and evaluate a working Q-Learning algorithm using Haskell. Q-Learning is a reinforcement algorithm that uses rewards to learn a mapping from state-action pairs to values. In the case of my project, the Q-Learning problem was framed in terms of an agent, or ``robot", moving around a grid picking up objects. The goal of the agent was to learn a mapping that would allow it to pick up the most objects during testing.
	\par One of the major components of this project involved improving on an earlier attempt at implementing the Q-Learning algorithm in Haskell. My earlier attempt attempted to use recursion to perform repeated actions during training and testing, and had problems with stack overflows. My implementation for this project made use of monads and the Kleisli composition operator instead of attempting explicit recursion, and was met with success.
	\par The other interesting aspect of my project involves a performance comparison between a (correct) implementation of the algorithm I wrote in Python, and the current Haskell version. Time-based testing indicates that the Haskell version outperforms the Python version in terms of execution time.
	
	
	\section{Related Work}
	
	\par The concept of this project came out of the Machine Learning course taught by Professor Melanie Mitchell. I took that course in winter quarter 2017, and my first attempt at the Q-Learning algorithm I wrote in Haskell (which I had partially learned the quarter before that in CS 320). Because of problems with that initial implementation, I ended up re-writing it in Python in order to have a working program to turn in. 
	
		\subsection{Q-Learning}\label{beefcake}
		
		\par Q-Learning is a reinforcement algorithm that trains an agent to learn to perform tasks in an environment \cite{RL}. In contrast to a static policy (e.g. state machine), which maps states to actions, the q-learning algorithm is concerned with approximating an optimal policy by mapping state-action pairs $(s, a)$ to real values. At a given timestep, the agent only has access to its current observable state together with a list of available actions from the given state. It must learn the best policy $\pi : S \rightarrow A$ to maximize some reward function $\delta : S \times A \rightarrow \mathbf{R}$.
		\par The strategy taken by Q-Learning is to approximate $\pi$ via a Q-function $Q(s,a)$, which not only maps state-action pairs to values, but generates those values from the assumption that the agent will follow the best policy into the future.
		\par The Q-function is formally expressed by $Q(s,a) = r(s,a) + \gamma \text{max}_{a^\prime} [Q(\delta(s,a), a^\prime)]$.
		\par Here, $r(s,a)$ represents the reward acquired from taking action $a$ at state $s$, $\text{max}_{a^\prime}$ represents the reward taken from the best possible action $a^\prime$ at the state $s^\prime$ produced by action $a$, and $\gamma$ represents a discount factor that allows the system to value future rewards lower than immediate rewards. The entire expression amounts to ``taking action $a$ at state $s$ gives some reward plus the discounted reward from the best future action." This gives the algorithm the ability to ``think ahead" and consider the long-term as well as the short term. We also include a learning rate, $\eta$, applied to the right side of the equation, when training. $\eta$ is the ``learning rate," i.e. the significance to which the algorithm gives any one ``lesson."
		\par In practice, the Q-function can be represented in several ways. Scientists and engineers at Google used a deep neural network for learning and representing the Q-function in their AlphaGo player \cite{alphago}. For all of the programs involved in this project, I have used a Q Table (also called a Q Matrix). One might think of the Q Table as 2-D array of reward values (Floats or Doubles), where the row indices are governed by a given state -- the five observable cells from a given grid position (north, south, east, west, and ``here"; diagonals are ignored) -- and the column indices governed by the five actions possible from an arbitrary state (move up, down, left, right, or attempt a pickup). In reality, I implemented the Q Table, in both the Haskell and Python versions, using an associative Map, but the array analogy is perhaps in nicer agreement with the theoretical details of Q-Learning.
		\par The Q-Learning algorithm, and indeed much of machine learning, is ultimately divided into two major phases: training and testing. The algorithm learns the Q-function during training via the following algorithm \cite{RL}:
		
		\begin{itemize}
			\item Observe state $s$.
			\item Select action $a$.
			\item Take action $a$ and receive reward $r$.
			\item Observe new state $s^\prime$.
			\item Update the Q Table; $Q(s,a) \Leftarrow Q(s,a) + \eta(r + \gamma \text{max}_{a^\prime}(Q(s^\prime, a^\prime)) - Q(s,a))$.
			\item Update $s \Leftarrow s^\prime$.
		\end{itemize}
		
		\par Action selection is governed by an ``$\epsilon$ -greedy" strategy, which seeks to strike a balance between exploration and exploitation. At the start of training, we initialize the parameter $\epsilon$ to 1. During action selection, the algorithm selects an action based on the probability of $\epsilon$. Therefore, initially every action is random -- exploration is maximized. The $\epsilon$ parameter is then ``annealed" over the training episodes: that is, every 50 episodes it is decremented by 0.01 until its value is 0.1. This means that at the other end, every 10th action (on average) taken by the fully trained algorithm will be random -- exploitation of the learned policy is favored over exploration, but exploration is never fully eliminated. As we will see below, this can sometimes allow the algorithm to break out of a bad policy. 
		\par Training is divided into steps and episodes. A single loop through the above algorithm represents a step; an episode consists of 200 steps, starting from a random location in a randomly-initialized board. During training, the algorithm runs through 5,000 episodes; while the boards are randomly reinitialized every episode, the Q Table is maintained from episode to episode, and ultimately passed out at the end of training for use during testing.
		\par For the purposes of this project, all starting boards have objects (``cans") in approximately 50\% of cells in an 8x8 board. Technically, the boards are all 10x10, with ``walls" on the outermost layers.
		\par During training, we gather metrics on the Q Table after each episode. Specifically, we are interested in the sum total reward stored in the Q Table after each episode. This allows us to visualize and understand the rate at which the algorithm learns, and its behavior during its attempt to maximize reward.
		\par The goal of testing is to acquire metrics used to evaluate the performance of the trained policy. The algorithm does not update its Q Table during testing; once training is done, the Q Table is static, although it still is passed along from testing episode to testing episode. Testing also runs 5,000 episodes of 200 steps each, randomly reinitializing the board state and agent location at the beginning of each episode. Testing is primarily concerned with reward acquired during each episode. There are many different ways to acquire and sift this and other types of data; for the purposes of this project, we collect the sum of rewards received for each episode during testing, then compute the average over the sums at the very end to evaluate the learned model.
		\par For a project of larger scope, it would have been interesting to instrument and compute statistics such as precision and recall, F-Score, or harmonic mean of rewards. Average reward over episodes only gives a very general basis for comparison between two models. Further metrics would allow us a more fine-grained understanding of where and why models performed as they did.
		\par In all implementations of training discussed in this project, the algorithm received a reward of 10 for a successful pickup of a can, a -5 penalty for colliding with a wall, a -1 penalty for attempting a pickup where there was no can, and no reward otherwise.
	
		\subsection{First Attempt}
		
		\par The initial design for my program was in some ways similar to the design for this project. I created alternative types for grid cells and for actions:\\
		
		\begin{lstlisting}[language=Haskell,numbers=none]
data Cell = Empty | Wall | Can | ERob | CRob deriving (Show, Eq)

data Act = U | D | L | R | P deriving (Show, Eq)
		\end{lstlisting}\ \\
		
		 \noindent Not unlike the implementation of the Dir type in my current program:\\
		
		\begin{lstlisting}[language=Haskell,numbers=none]
data Dir =  U
	 | D
	 | R
	 | L
	 | P
	 deriving (Show, Eq, Enum, Ord)
		\end{lstlisting}\ \\
		
		\par During my first attempt, I tried to store the grid and a given robot state as heavily parameterized types. In retrospect, I was trying to force an imperative ``struct" style that ended up being very awkward to handle:\\
		\begin{lstlisting}[language=Haskell,numbers=none]
data Grid = Grid{ cells :: [[Cell]]
	,   loc     :: (Int, Int)
	,   reward  :: Float
	,   step    :: Int
	,   qtable  :: Map String [Float]} deriving (Show, Eq)

data State = State{ north :: Cell
                  , south :: Cell
                  , east  :: Cell
                  , west  :: Cell
                  , here  :: Cell
                  , key   :: String} deriving (Show, Eq)

		\end{lstlisting}\ \\
		
		\par The other major mistake in my old program was my attempt to use explicit recursion to perform looping over steps and episodes. I knew that a traditional use of the stack to do this would not be sufficient, that it would blow up at some point, but I was also aware of Haskell's capability to optimize use of the stack out of functions utilizing tail recursion -- my first program was based around the gamble over whether or not I could achieve tail recursion, and/or use it effectively.
		\par Here we have an example from my first program that attempts to execute many training episodes using this explicit recursion:
		
		\begin{lstlisting}[language=Haskell,numbers=none]
		
execEpisode :: [Act] -> [Bool] -> Int -> Grid -> Grid
execEpisode rands isRands i grd = let s = getState grd
	act = getNewAction rands isRands grd
	grd' = move act grd
	s' = getState grd'
		in if i == m_global
			then learn s s' act grd'
			else execEpisode rands isRands (i+1) 
				(learn s s' act grd')

		\end{lstlisting}\ \\
		
		\par Part of the problem here (I think) is lazy evaluation of all the parameters being passed into the recursive call. It may be that forcing these evaluations prior to entering/evaluating the next call would have obviated some of the disaster that I experienced (in the form of stack overflow). It could also be that this function is not truly tail recursive, although I cannot see where. Furthermore, this is not the only function in the program designed this way, so it may not be the culprit. 
		\par My lack of understanding of IO and Monads in general prevented me from wanting to spend a lot of time working up tests last quarter, and I decided that the best use of my time would instead be to implement the program in a language I already knew.
		
		\subsection{Python Version}
		
		\par The working program that I eventually wrote in Python took an object-oriented approach. It implemented a class each for a grid cell, the grid/board itself, the Q Table, and the algorithm manager. A cell maintained its status, which of course could be updated in an imperative manner. The board object instantiated the grid explicitly as a 2-D array of cells and maintained state used for bookeeping and tracking of objects on the grid. The Q Table object acted as a domain-specific interface to a Python dict where the pair-value mappings were kept. And the algorithm manager held state and implemented the methods necessary for carrying out the algorithm defined in section \ref{beefcake}.
	
	\section{Project Design \& Implementation}
	
		\subsection{Board Representation}
		
		\par My current program diverges from both the previous Haskell attempt and the working Python program in important ways, but it also borrows a fair amount from both prior versions.
		\par Instead of explicitly representing the board environment as the grid defined by the robot problem description, I chose to track the can locations (as a list of Can types), the robot's location (as a single Rob type), and the dimensions of the grid (as a tuple of Ints) separately. Whether this saves space seems debatable. What it allowed me to do was throw away the explicit Cell object from prior versions, and depending on language implementation and internal object state, the amount of memory required for a full 2-D array or list could be non-trivial. 
		\par Performance-wise this also seems like a debatable trade-off. One advantage to not having Cell objects is not incurring the time required for instantiating each of these memory objects. However, in an imperative program, this instantiation need happen only once per episode; so saving on a constant-time operation is probably not that big of a get. We also lose the constant-time array access when observing state or checking moves. However, for a grid of small size, because we know that only around half of the cells will have Cans on them, searching cans is a relatively trivial operation -- although it is technically linear WRT board size, with the board size used in this project it might as well be constant time. 
		\par An additional advantage involves the functional paradigm: if we were to track every cell in the grid, we would be required to regularly rebuild the grid as cans were picked up. Because the functional paradigm prevents us from simply updating the state of a cell, heavy list processing would be involved in swapping a cell containing a can out for an empty cell. Likewise for whenever the robot moved from one cell to another. Instead, with the new implementation, when the robot moves we can simply replace the Robot parameter in the board -- a constant time operation -- or, when a can is picked up, cut an item out of the cans list -- a linear time operation for which the language is well-suited.
		\par Although for an imperative language, a 2-D array seems like a great way to go, in this case it appears to be a comparison between ``apples and oranges." Design choices appropriate for one do not necessarily carry over to the other.
			\par Imperative to the training and testing algorithm described in section \ref{beefcake} are randomized board states and robot locations. This was one of the major challenges during my first attempt at writing this program in Haskell -- it had not occurred to me that random number generation would violate the assumptions of purity of some standard function. IO and Monads in general were poorly-understood concepts to me at the time, and I believe not having a solid understanding of them is what ultimately prevented me from getting the program working the first time.
		\par My new design choices about board representation also helped me here, however. Instead of having to decide for each cell whether it should be empty or have a can on it, or whether the robot should start there, I could instead simply decide what percentage of the total cells should have cans, and generate a list of cans by generating random pairs within some constrained range. This ended up allowing me to isolate IO behavior much more easily.

		\subsection{Learning}
		
		\par For observing its state, the agent needs a way of differentiating the condition of one state from that of another. For this, my solution was very similar to both prior versions: having some notion of the contents of a cell, described by an alternate type (the State type in the code). Furthermore, for quick storage and retrieval of a given state in the Q Table, we need some way of generating a unique "key" from an observation. My choice for this also followed previous versions: a length-5 text string, generated programmatically, representing each possible observable state.
		\par Action selection was governed by the $\epsilon$-greedy policy -- first generating a random number to assert randomness or informed selection, and then selecting an action based on the results. Selecting a random action was accomplished by yet another random number generation. Selecting the best action required a lookup of the current state from the Q Table and determining which action ``column" in the table held the highest value. 
		\par A training step in my program simply transforms one Q Table, Board pair into another via a move action and its resulting reward computation. Repetition of this process over 1 million steps (5,000 episodes of 200 steps each) incrementally fills out the Q Table and models a ``best" heuristic for the agent's behavior on the grid.
		\par The function updating the Q Table was the one most prone to error. It required ``unpacking" two Maybe types from Map lookups, and constructing new lists of values from the Q function to replace the original lookup value. Handling the possibility of ``bad lookups" perhaps should not have been strictly necessary, given a sufficiently correct Q Table build algorithm, but seemed prudent nevertheless.
		\par The Q Table build function -- \texttt{newQTable} -- built a blank Q Table by filling an associative map with strings representing all possible combinations of state values. This process produced state keys that would never actually be seen in practice, such as state strings containing three or more walls, but the amount of superfluous memory wasted by these results was outweighed by the ease of writing and verifying the process.
		\par Executing a test step ended up being very similar to executing a training step, with the major difference being treatment of the Q Table. The Q Table is updated each training step, but remains static throughout testing. The function \texttt{test} still takes and produces a QTable, Board, Double triple, however, because it must be able alter the \textit{board} and pass everything on to the next test step when composed with the Kleisli operator. The Double value taken and produced by the \texttt{test} function represents the cumulative reward obtained throughout a test episode. 
		
		\subsection{Executing the Algorithm}
		
		\par Having training and test steps working is all well and good, but ultimately not enough. This was the point at which I became stuck during my first attempt at a Haskell program. Specifically, the challenge was how best to accumulate or capture the updated board and Q Table from step to step, passing the output of one step into the next in an efficient way. I was eager to stay away from explicit recursion, as it had landed me in a poor situation during my first attempt.
		\par The answer ended up being the Kleisli Composition operator -- \texttt{(>=>)} -- from the Control.Monad library \cite{kleisli}. \\
		
		\begin{lstlisting}[language=Haskell,numbers=none]
(>=>) :: Monad m => (a -> m b) -> (b -> m c) -> a -> m c
(m >=> n) x = do
	y <- m x
	n y
		\end{lstlisting}\ \\
		
		This operator takes the monadic output from one function and feeds it into the next as a non-monad, returning a function from the input of the first to the output of the second. The behavior of this operator is associative.
		\par It turned out that this was exactly the operator I needed to get my program working. My episode functions made use of it by first generating lists of \texttt{train} or \texttt{test} functions, partially applied to their configuration/control arguments, and then folding the Kleisli operator over the list, beginning with an initial board and Q Table pair. It was then easy to produce many episodes by repeating this process from a \texttt{doTraining} or \texttt{doTesting} wrapper function. 
		\par This was the breakthrough I needed! Thanks to Professor Jones for helping me out with this one.
		
		\subsection{Instrumentation}\label{inst}
		
		\par An additional problem presented itself in the form of an instrumentation problem. How could I exflitrate the metrics that I needed to test and observe my algorithm? What I needed, at the very least, was the total reward content stored in the Q Table after each training episode, as well as the total reward obtained by the agent during each testing episode. The first metric would give me an idea of how well the algorithm was ``learning." By watching the total contents of the Q Table rise or fall, I could tell if the algorithm was making a good distribution of moves, and if its efficiency was improving over training episodes. The second metric would give me an idea of how well the \textit{trained} algorithm performed overall during testing.
		\par The solution to this problem ended up being a matter of passing reward values through the test and train functions, and in turn the episode functions, along with the board and Q Table states. 
		\par I am not totally satisfied with this solution, however, as it is very inflexible on account of being ``hard-coded" into the algorithm. It would be very tedious to add further instrumentation to collect metrics in this way.
		\par What could be a better possible solution? Maybe it would involve writing to a file during each step? This seems like it would be far too slow. Maybe create a parameterized type to hold several different types of values, and include some accompanying function for filling the type? This seems the likely answer; although it would still require hand-coded adjustments when new instrumentation needed to be added, at least these changes would only need to take place in just one place. 
		\par However, my worry with this solution would be the memory management requirements involved in building a new, complex parameterized type for each step. Could we simply introspect the functions as they execute from the outside, perhaps from a parallel thread? I still don't have a great answer to this question.
		\par In the end, the instrumentation that made it into this project represents ``minimum viable product" in that it afforded me the bare minimum of metrics required to have a basic assessment of the algorithm, but provides very little in the way of convenience beyond that. 
		
			
	\section{Testing \& Results}\label{testingresults}
	
	\par In this section I'll discuss my process of testing my program, comparing it to a Python version, and collecting and interpreting the data it generated.
	
		\subsection{Testing}
		
			\subsubsection{Correctness}
			
			\par Testing for this program was done through manual inspection of outputs. When testing, I checked output values against expected values, and also used output values in assessment of ``expected behavior" of the algorithm in the context of my previously written working program.
			\par Of utmost importance was board generation, and agreement between the formal board parameters and the visual representation of the board. We can start small and manually check these values: \\
			\begin{lstlisting}[language=Haskell,numbers=none]
*Qlearn> b <- randBoard (4,4) 0.5
*Qlearn> b
Board ((4,4),[Can (2,1),Can (3,2),Can (1,2),Can (2,0),Can (3,1),Can (3,3),
Can (1,3)],Rob (0,0))
*Qlearn> showBoard b
[ ][.][.][.]
[.][.][ ][ ]
[ ][ ][.][.]
[o][ ][ ][ ]
			\end{lstlisting}\ \\
			
			\par Here we expect that the all the Can and Rob coordinates lie inside the board dimensions, i.e. $0 \leq i < dim$. We also expect there to be a number of cans approximately equal to the floating point value passed as the second argument to \texttt{randBoard}, in this case $0.5$. I use the word ``approximately" here because of two reasons: 1. integer values means an exact percentage may not be possible, and 2. the cans are randomly generated and duplicates may occur (in which case they are pruned out by the algorithm).
			\par Furthermore, we expect the display generated by \texttt{showBoard} to agree with the formal contents of the Board. Hand-verifying in small cases like these is relatively easy, but also slow and based on my own perceptive abilities. Given more time, automated testing could have been implemented to attain wider coverage.
			\par Keep in mind that the first value in a tuple represents its $i$ value, or location on the $y$-axis, and the second value represents its $j$ value, or location on the $x$-axis. This is slightly clunky from an array manipulation, computer science perspective, but fits nicely into the visual implementation of a grid as a list of lists. It also agrees with mathematical treatments of matrices.
			\par We can scale this up; the manual examination becomes more tedious but at least we have some assurance that the program is behaving as expected:\\
			
			\begin{lstlisting}[language=Haskell,numbers=none]
*Qlearn> b <- randBoard (8,8) 0.2
*Qlearn> b
Board ((8,8),[Can (1,4),Can (7,0),Can (2,0),Can (0,0),Can (1,5),Can (0,2),
Can (4,6),Can (6,2),Can (7,2),Can (4,0),Can (2,2),Can (0,3)],Rob (0,6))
*Qlearn> showBoard b
[.][ ][.][ ][ ][ ][ ][ ]
[ ][ ][.][ ][ ][ ][ ][ ]
[ ][ ][ ][ ][ ][ ][ ][ ]
[.][ ][ ][ ][ ][ ][.][ ]
[ ][ ][ ][ ][ ][ ][ ][ ]
[.][ ][.][ ][ ][ ][ ][ ]
[ ][ ][ ][ ][.][.][ ][ ]
[.][ ][.][.][ ][ ][o][ ]
			\end{lstlisting}\ \\
			
			\par Here we have 12 cans ($12.8$ being the exact figure for $(8 \times 8) \times 0.2$), and we can see that all coordinates fall within the given dimensions.
			\par We are going to ignore some edge cases here, such as board dimensions $(0,0)$, because such a case would be outside the problem domain -- i.e. it would make no sense and be of no use to train a Q-Learning agent on a board with no squares. 
			\par Additionally, I was able to use the \texttt{showBoard} function to verify that the agent's movements on the board were happening as expected. I won't reproduce the output here for space considerations, but one can see in the code appendix where the print statements facilitating this kind of inspection have been commented out. This is another feature I would have liked to have had the ability to toggle on and off from a terminal, but again, this feature was cut out due to time constraints.
			\par Testing correctness of the Q Table update function was achieved through gathering and plotting the metrics described in section \ref{inst}. Because it is very difficult to look at the printout of a Q Table -- all of its keys and values enumerated in sequence -- and be able to tell correctness from incorrectness, it was necessary instead to perform a more qualitative kind of verification in this case. Results from this type of testing are shown and discussed later on in section \ref{testingresults}.
			\par Automated testing for this program ended up being an insurmountable obstacle, given the scope and time constraints of the project. 
			\par I made a foray into writing properties that would be checkable with Quick Check, and was led toward Monadic checks (e.g. \texttt{forAllM}) because of the nature of my program. My goal was to write checks along the semantic lines of ``the cans and agent of any randomly-generated board should always fall inside its dimension values," and maybe ``the updated value in any Q Table should be equivalent to the Q function applied to the original value." 
			\par The barrier to the first type of check was my lack of understanding: the Quick Check examples provided in class did not address testing IO functions that I could tell, and the literature online was both wide and deep. 
			\par The barrier to the second type of check was limited time and scope, from an engineering standpoint. Writing such tests manually, without the assistance of an automated tool such as Quick Check, would have taken a lot of time that I didn't have and would have been at least as error prone as the functions they were meant to test.
			
			\subsubsection{Behavior \& Performance}
			
			\par I was interested in comparing my new program with my previously written working program, written in Python. I knew that the Python version was correct (or at least sufficiently correct to receive an A in the Machine Learning course), so a comparison between the data gathered from both programs would help me get an idea of whether my new program was correct as well. Additionally, I was curious to see how the two programs compared in terms of time required to execute the same training/testing sequence.
			\par Both programs report their results by generating a graphical plot in the form of a pdf or png. This would be the way in which I would gather information about the behavior of each program. See these results and discussion in sections \ref{data} and \ref{interp}.
			\par For comparing performance, I conducted some tests by running each program from a bash shell composed with a call to the bash utility \texttt{time}. The programs were both run on the same 2016 Macbook with a dual core Intel m3 processor and 8 GB of RAM. The Haskell program was first compiled using GHC. Both programs train an agent on a 10x10 board (where walls make up the outermost cells) for 5,000 episodes of 200 steps, then test the trained agent for 5,000 episodes of 200 steps. Here are the initial results:\\
			
			\begin{lstlisting}[numbers=none]
Codys-MacBook:qlearning cody$ time ./main
Training...
Training finished.
Testing...
Testing finished.

real	0m48.034s
user	0m47.207s
sys	0m0.701s
Codys-MacBook:qlearning cody$

Codys-MacBook:qlearn cody$ time python qlearn.py

real	1m37.250s
user	1m36.285s
sys	0m0.739s
Codys-MacBook:qlearn cody$
			\end{lstlisting}\ \\
			
			\par It would appear that the Haskell program is much faster than the Python. However, we have a wildcard in this comparison: namely, the library calls performing the image generation. Maybe the library calls are skewing the comparison? This seemed to warrant running both programs again without the image generation step at the very end:\\
			
			\begin{lstlisting}[numbers=none]

Codys-MacBook:qlearning cody$ time ./main
Training...
Training finished.
Testing...
Testing finished.

real	0m43.698s
user	0m42.876s
sys	0m0.608s
Codys-MacBook:qlearning cody$

Codys-MacBook:qlearn cody$ time python qlearn.py

real	1m39.152s
user	1m38.211s
sys	0m0.715s
Codys-MacBook:qlearn cody$
					\end{lstlisting}\ \\
					
					\par This seems to have not made any difference. 
					\par Maybe it comes as no surprise that a compiled program runs faster than an interpreted one. And it could very well be that some of the design choices I made in the Python version were not the best. I've never had formal instruction on Python, after all. And maybe comparing the performance of the two languages in this way tells us very little, since the programs were not written using the same abstractions or algorithms. Still, the results are interesting and encouraging -- using Haskell has paid off in terms of performance, in this case!

		\subsection{Data Collected}\label{data}
		
			\par As discussed above, the primary metric of concern when running this program was the rewards accumulated over all training episodes, and the average rewards obtained from each testing episode. These were then plotted on a graph. Results varied a bit from experiment to experiment, but here is a typical example:\\
			
			\includegraphics[width=\textwidth]{rewards_eps0_1}
			
			
		\subsection{Interpretation of Data}\label{interp}
		
		\par At the top of the image, the average reward acquired over all testing episodes is listed. This gives us some idea of how well the agent did in a given episode during testing -- in this case we might say that it was able to pick up around 12 of the 30-ish cans on the randomized board of any given episode. This might not be totally accurate, as it does not account for ``mistakes" such as attempting pickups without cans being present, or attempting to move into walls, but it gives us some idea.
		\par The plotted line shows the sum of all values stored in the Q Table at the end of a given training episode. In some ways this gives us an idea of when most of the learning takes place during training -- we see that the algorithm learns most of the particulars of its policy by the 1,000th episode, and merely finetunes thereafter.
		\par We can compare this output to the output generated by the old Python version:
		
		\includegraphics[width=\textwidth]{python.pdf}
		
		\par This comparison is interesting. We can see that the rewards are topping out at approximately the same values, but that the Python version appears to go on to continue to "make mistakes" during training, thereby decreasing its overall stored value. However, the average reward received during test episodes for the Python version on this experiment was $143.2$, perhaps leading to the conclusion that the Python version has learned a better model. 
		\par I am actually not sure about the cause of the discrepancy between the two plots. I suspect it has to do with differences in the Q Table update functions between the two programs, or perhaps in an incorrect implementation of moving and move checking in the Python version. Other than having received an A for the Python program, I don't have a strong baseline program, that is known to be completely correct, against which to compare. Extensive experimentation, and perhaps rewriting of the Python version, will be required to discover the source of this incongruity.
		\par At the very least, we can see that the Haskell program is indeed ``learning," i.e. filling its Q Table with learned values and using them during testing to execute a better-than-random behavior policy. However, the metrics collected give us only a basic idea of what is going on with the algorithm. Truly understanding the model generated or derived by a machine learning algorithm requires lots of experimentation as well as thinking about precisely how to measure qualitative behavior. This task is perhaps outside the scope of this paper, and actually remains an area of open research \cite{nguyen}.
			
	\section{Reflections}
	
		\subsection{Major Challenges}
		
		\par The biggest hurdle to getting this program working, from the perspective of my first attempt, was having a good understanding of IO with Haskell and Monads in general. The solution to that hurdle, of course, was to take the Functional Languages class, and learn about how Monads are themselves like programs that produce outputs, or potentially have side effects. 
		\par Additionally, it was my goal to design the program in a way appropriate to the functional paradigm. I don't think that I accomplished this during my first attempt: using heavily parametrized types as stand-ins for struct-like objects and attempting to ``loop" using explicit recursion were my two big failures here. They made my job as a functional programmer more difficult, and in some ways occluded the way toward a functionally-appropriate answer. Redesigning the program with a better understanding of Haskell helped me to keep the functional idioms in mind and ultimately to have an easier time of finding solutions to the small hangups I encountered.
		\par Finally, the ``hump," as it were, was finding a way to pass the output of one Monadic function as non-monadic input to another, accomplished using the Kleisli operator. Thanks to Professor Jones for his guidance here! After this problem was solved, writing the rest of the program was easy.
		
		\subsection{Haskell vs. Python}
		
		\par I don't believe that my ``performance comparison" between the two programs represents definitive proof that Haskell is faster than Python, at least any more than do the observations that the Haskell program was compiled, and the language is typesafe, whereas the Python program is interpreted, and Python performs lots of type conversions at runtime. 
		\par To acquire a true ``apples to apples" comparison would require that the two programs be written in as close to the same way as possible, using the same abstractions and algorithms and design patterns. As this may not always be possible, care would be required when choosing language-specific alternatives. 
		\par Furthermore, the execution environment would need to be more controlled. With a two-core processor running lots of other applications simultaneously, there could have been interference at the hardware or kernel level in the experiments I conducted. Granted, the times I observed are reproducible on my machine and do not show a lot of variance, but from a scientific standpoint the execution environment is still a variable that requires controls. 
		\par The big takeaway from the comparison for me was an assurance that, even with all the linear-complexity list processing tasks such as folds, maps, filters, and finds, the Haskell program performs at least as well, in an asymptotic sense, as a program that utilizes O(1) array access and imperative-style side effects. I think that this kind of assurance is essential to have for any language one uses in practice, and being able to do this kind hands-on experimentation is a great way to reinforce that assurance.
		
		\subsection{Quality of Solution \& Improvements Over First Attempt}
		
		\par Is my solution the best possible implementation of the Q-Learning algorithm in Haskell? Almost certainly not. It seems likely that there is a more efficient way to represent the board and its pieces, such as perhaps a bitboard (where each grid square is represented by a bit, meaning the entire board can fit inside a single word or double-word). It might also be worth it to find or write a data structure for the Q Table whose lookup routine does not return a Maybe type that requires unpacking -- if we were to build such a data structure with assurances of complete state coverage, we would not need to handle the possibility of null returns. Probably the algorithm for observing the current state could be optimized to perform only a single pass over the cans list, rather than five separate passes for each relevant cell.
		\par Additionally, to make such a program truly useful for thorough experimentation, it would be nice to have some kind of terminal for parameterizing, dispatching, and gathering data from experiments. Exploration of the algorithm becomes interesting when one starts to fiddle with the learning rate, the epsilon value, the different rewards for each move, and the number of steps and episodes executed. Changing all these parameters manually in the code base is obviously not the way to go, but building a terminal for this program would require further thinking about function parameters, possibly global values, and a major redesign with these types of adjustments in mind. 
		\par Ultimately this program represents a major improvement over my first attempt. This might be obvious, because this program works and the other one did not. However, I think the major win here is that the newer program has been written with the functional paradigm in mind, and shows evidence (at least to me) of a more mature understanding of the particulars of Haskell and functional programming in general. My attempts in writing the first program to bring some imperative abstractions into the functional world were likely the true barriers to success in that endeavor. Writing this program successfully has provided a great example of how to adapt one's thinking, ``switch gears" so to speak, and consider the implementation of a computing problem from more than one angle. I find that I now have a new set of tools with which to consider a problem and build an appropriate solution. This feels like a major step in my development as a computer scientist.

	\pagebreak
	\begin{thebibliography}{9}
		\bibitem{RL}
		Melanie  Mitchell.
		\textit{Reinforcement Learning}.
		http://web.cecs.pdx.edu/~mm/MachineLearningWinter2017/RL.pdf
		\bibitem{kleisli}
		Monad laws.
		https://wiki.haskell.org/Monad\_laws
		\bibitem{nguyen}
		Nguyen A, Yosinski J, Clune J. 
		\textit{Deep Neural Networks are Easily Fooled: High Confidence Predictions
for Unrecognizable Images.}
		In Computer Vision and Pattern Recognition (CVPR ?15), IEEE, 2015.
		\bibitem{alphago}
		Silver, D., A. Huang, Aja, C. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, D. Hassabis. 2016. \textit{Mastering the Game of Go with Deep Neural Networks and Tree Search}. Nature 529 (28 Jan): 484--503.
	\end{thebibliography}
	
	\pagebreak
	\appendix
	\section{Github}
	
	https://github.com/codyshepherd/qlearn\_functional
	
	\section{Haskell Code}
	
	\begin{lstlisting}[language=Haskell,numbers=none,basicstyle=\tiny]
Board.lhs
Cody Shepherd

> module Board where
> import Data.List
> import Math.Geometry.Grid
> import Math.Geometry.Grid.Square



The Rob type is an instance of Rob's location on the board. We would like to 
be able to compare this location to a pair of Ints later on, so we will provide a couple
of conversion functions.

> data Rob = Rob (Int, Int)
>               deriving (Show, Eq)

> rfst      :: Rob -> Int
> rfst (Rob (a, b)) = a

> rsnd      :: Rob -> Int
> rsnd (Rob (a, b)) = b

> rToPair   :: Rob -> (Int, Int)
> rToPair (Rob a) = a



The first thing Rob has to do is move around a board. 
It seems like we don't need to store all the empty locations... we can just
track where there are cans. A can, like Rob, is defined by its location.

A location will be defined in its implementation as (a, b), where a represents the "row"
or y-axis location, and b will represent the "column" or x-axis location. This is reversed
from the format of cartesian coordinates because this version makes it much easier to 
use in the context of lists of lists - a being the number of the outermost list, and b 
being the offset within that list. 

However, this format will behave as expected (as a cartesian pair) when visualized. I.e. specifying
Rob's location at (0, 0) will put him at the bottom left corner of the grid, and moving him up will
result in a location of (0, 1). This should turn out to not matter much in the actual execution of
the q-learning algorithm, as it is primarily concerned with the "key" of each grid square rather 
than its grid coordinates.

We will specify the locations of Cans in much the same way.

> data Can = Can (Int, Int)
>            deriving (Show, Eq)

> cfst       :: Can -> Int
> cfst (Can (a, b)) = a

> csnd          :: Can -> Int
> csnd (Can (a, b)) = b

> cToPair       :: Can -> (Int, Int)
> cToPair (Can a) = a


For the board, we will have to define limits, but otherwise we don't need to store 
empty cells.

We will define Board as a 3-tuple with its height and width limits, plus a list
of cans and the location of Rob: ((MAXHEIGHT, MAXWIDTH), Cans, Rob)

> data Board = Board ((Int, Int), [Can], Rob)
>               deriving (Eq, Show)

> bfst      :: Board -> (Int, Int)
> bfst (Board (a, b, c)) = a

> bsnd      :: Board -> [Can]
> bsnd (Board (a, b, c)) = b

> bthd      :: Board -> Rob
> bthd (Board (a, b, c)) = c

> top       :: Board -> Int
> top (Board (a, b, c)) = (fst a) - 1

> right     :: Board -> Int
> right (Board (a, b, c)) = (snd a) - 1



If Rob is going to move, we would like some way of constraining his possible movements
to the cardinal directions. We'll define a direction in the order of "nsew", though it 
doesn't make any difference as long as we are consistent throughout the program.

> data Dir =  U
>           | D
>           | R
>           | L
>           | P
>           deriving (Show, Eq, Enum, Ord)

At some point we will need to convert a Dir (which represents an action) to an index for
getting values out of the Q Table.

> dirIndex  :: Dir -> Int
> dirIndex d
>           | d == U = 0
>           | d == D = 1
>           | d == R = 2
>           | d == L = 3
>           | d == P = 4

We want a nice way to visualize the board.

First we need to break up a list of "[ ]" cell tokens into a list of n cell tokens, delimited
by newlines:

> splitEvery :: Int -> [String] -> [String]
> splitEvery _ [] = []
> splitEvery n s = take n s ++ "\n" : splitEvery n ss
>                   where ss = drop n s

Then we will process the list of cans into their places on the "grid" and build a graphical
representation of the grid for printing.

> showBoard     :: Board -> IO ()
> showBoard b     = do let r = (rfst (bthd b), rsnd (bthd b))
>                          cs = map (\x -> if any (\y -> (cfst y == fst x) && (csnd y == snd x)) (bsnd b) && (r == x)
>                                      then "[%]"
>                                      else if any (\y -> (cfst y == fst x) && (csnd y == snd x)) (bsnd b) 
>                                           then "[.]"
>                                      else if r == x
>                                           then "[o]" 
>                                      else "[ ]") (indices (rectSquareGrid (fst (bfst b)) (snd (bfst b))))
>                          ls = splitEvery (snd (bfst b)) cs
>                          s = reverse (lines (concat ls))
>                      r <- return s
>                      mapM putStrLn r
>                      putStrLn ""
>                      return ()


The very first thing we need to work out is movement around the board.
A movement is an action performed on Rob - none of the other pieces move.
Rob's movement does not affect any of the state on the board other than 
Rob (with the exception of a pick-up movement).

> moveRob      :: Rob -> Dir -> Rob
> moveRob (Rob (x, y)) d  = case d of
>                   U -> Rob (x+1, y)
>                   D -> Rob (x-1, y)
>                   R -> Rob (x, y+1)
>                   L -> Rob (x, y-1)
>                   P -> Rob (x, y)

In the Q-Learning problem, a move is conflated with a reward. Every move returns some
notion of a reward, even if the value of the reward is zero. 

A move is also essentially a permutation on a board, so we will need to return the updated
board as well.

The robot is rewarded 10 points for picking up a can, and is penalized 1 point for attempting to
pick up a can when none is present, and 5 points for running into a wall. 

Note that here we must make sure that if the robot chooses to move into a wall, it "bounces back,"
i.e. b' is identical to b for starting board b.

> move          :: Dir -> Board -> (Board, Double)
> move dir (Board (dims, cans, rob)) 
>                                  | r' == r = case find (\c -> cToPair c == r) cans of
>                                                   Just a -> (Board(dims, filter (\c -> cToPair c /= r) cans, rob), 10.0)
>                                                   Nothing -> (Board(dims, cans, rob), -1.0)
>                                  | otherwise = 
>                                             if (fst r' > (fst dims) -1 )      ||
>                                                   (fst r' < 0)                ||
>                                                   (snd r' > (snd dims) -1)    ||
>                                                   (snd r' < 0)        
>                                               then (Board(dims, cans, rob), -5.0)
>                                               else (Board(dims, cans, Rob r'), 0.0) 
>  where
>       r = rToPair rob
>       r' = rToPair $ moveRob rob dir

=============================
	\end{lstlisting}
	
	\begin{lstlisting}[language=Haskell,numbers=none,basicstyle=\tiny]

Learning.lhs
Cody Shepherd

> module Learning where

> import System.Random
> import Data.List
> import Data.List.Extras.Argmax
> import Data.List.Utils
> import Data.Ord
> import Board
> import Data.Map (Map)
> import qualified Data.Map as Map


This will be the module that defines the learning algorithm - i.e. the math part.

It will define the functions responsible for creating and updating the Q-Matrix

First, we should define the Q-Matrix and specify how one stores, retrieves, and updates
information contained within it.

The state of a cell is going to be important in the Robot's learning algorithm. The robot needs
to be able to observe the contents of its cell and neighboring cells in order to make a
decision about the best course of action.

Therefore we create named values for different configurations of a state.

> data State    = E
>               | C
>               | W 
>               | Rc
>               | Re
>               deriving (Eq, Show, Ord, Enum)

We also are going to want a quick way to create a "hash" of a given state signature for being
able to store information about a state configuration in the Qmatrix.

> stateKey          :: [State] -> String
> stateKey []       = []
> stateKey (x:xs)   = let nx =  case x of
>                                   E -> '_'
>                                   C -> '.'
>                                   W -> 'w'
>                                   Rc -> '%'
>                                   Re -> 'o'
>                       in nx : stateKey xs

Our Qmatrix is just a simple Mapping

> type Qmatrix = Map String [Double]

Now that we have a notion of a state and state key (a stateKey string plus a list of five Double values),
we need a function that allows Rob to observe his current state.

I have chosen to break this observe function into four separate functions to make it easier to manage
and understand. Ultimately an "observation" results in a stateKey string representing the configuration
of 5 cells that make up the robot's "footprint."

> obsU        :: Board -> State
> obsU (Board ((a, b), cs, r))
>                            | fst rx == a-1 = W
>                            | any (\x -> (cfst x == (fst rx)+1) && (csnd x == snd rx)) cs = C
>                            | otherwise = E
>                            where 
>                               rx = rToPair r

> obsD        :: Board -> State
> obsD (Board ((a, b), cs, r))
>                           | fst rx == 0 = W
>                           | any (\x -> (cfst x == (fst rx)-1) && (csnd x == snd rx)) cs = C
>                           | otherwise = E
>                           where
>                               rx = rToPair r

> obsE          :: Board -> State 
> obsE (Board ((a, b), cs, r))
>                           | snd rx == b-1 = W
>                           | any (\x -> (csnd x == (snd rx)+1) && (cfst x == fst rx)) cs = C
>                           | otherwise = E
>                           where
>                               rx = rToPair r

> obsW          :: Board -> State 
> obsW (Board ((a, b), cs, r))
>                           | snd rx == 0 = W
>                           | any (\x -> (csnd x == (snd rx)-1) && (cfst x == fst rx)) cs = C
>                           | otherwise = E
>                           where
>                               rx = rToPair r


> obsH          :: Board -> State 
> obsH (Board ((a, b), cs, r))
>                           | any (\x -> (csnd x == snd rx) && (cfst x == fst rx)) cs = Rc
>                           | otherwise = Re
>                           where
>                               rx = rToPair r

> observe       :: Board -> String
> observe b   = let  n = obsU b
>                    s = obsD b
>                    e = obsE b
>                    w = obsW b
>                    h = obsH b
>                    in stateKey [n, s, e, w, h]

Fundamentally, Q-Learning requires two parts: training and testing. These parts differ
in that the q-matrix may be updated during training, but not during testing.

Training 
    - Initialize Q(s,a) to all zeros
    - Initialize s
    - selection action a
    - take action a and receive reward r
    - observe new state s'
    - Update Q(s, a) <- Q(s,a) + eta * (r + gamma*argmax(Q(s',a')) - Q(s,a))
    - s <- s'

testStep
    - The same steps as above except without updating the Q Table

In service of either algorithm, the robot needs to be able to select an action 
in a deterministic way, but also get a random action if it wants.

> action :: Int -> IO Dir
> action n =  case n `mod` 6 of
>                0 -> return U
>                1 -> return D
>                2 -> return R
>                3 -> return L
>                4 -> return P
>                5 -> do v <- randomRIO(0,4)
>                        action v

Sometimes this action selection should be random (e.g. when the robot has not learned enough,
or when it wants to maximize exploration over exploitataion). The chances of randomness should be 
based on a tunable value.

> isRandom :: Double -> IO Bool
> isRandom p = do v <- randomRIO(0.0, 1.0)
>                 if v < p then return True else return False

We also want a way to get an action we know or think is good. This requires looking at the Qmatrix
and checking for any "learned knowledge" for the given stateKey state configuration.

> maxI :: [Double] -> Int
> maxI xs = let (f, i) = maximumBy (comparing fst) (zip xs [0..]) in i

> bestAction :: String -> Qmatrix -> IO Dir
> bestAction k q = do let l = Map.lookup k q
>                     --print ("bestAction lookup: " ++ show l)
>                     case l of
>                            Just n -> action (maxI n)
>                            Nothing -> action 5

In order to perform a single step during training, we need to have the updated states of
the Board and the Qtable; a single step potentially updates both, so it should return them,
I suppose as a pair.


We will need some global constants to control the behavior of our program and keep the number of
copied parameters to a minimum.

> eta = 0.2 :: Double
> gamma = 0.9 :: Double

Training is fundamentally an updating of the board position and the contents of the Qmatrix. Because it
relies on randomness, it becomes an IO function. 

> train         :: Double -> (Qmatrix, Board) -> IO (Qmatrix, Board)
> train eps (q, b) = do         t <- isRandom eps
>                               let s       = observe b
>                               --showBoard b
>                               a <- if t then action 5 else bestAction s q
>                               --print a
>                               let (s', r) = move a b
>                                   q'      = updateQ q s (observe s') a r
>                               --print ("reward " ++ show r)
>                               --print ("stateKey: " ++ show s)
>                               --print (Map.lookup s q')
>                               return (q', s')

The Qmatrix must be updated according to the Q-Learning algorithm. A more detailed explanation of this algorithm,
and thus what is going on in this function, is provided in my paper. 

> updateQ           :: Qmatrix -> String -> String -> Dir -> Double -> Qmatrix
> updateQ q s s' a r   = let qcurrent = Map.lookup s q
>                               in case qcurrent of
>                                   Just sa -> let y = head $ drop (dirIndex a) sa
>                                                  snext = Map.lookup s' q
>                                                  qt = case snext of
>                                                               Just z -> let newval = y + (eta * (r + (gamma * (maximum z)) - y))
>                                                                           in take (dirIndex a) sa ++ [newval] ++ drop ((dirIndex a) + 1) sa
>                                                               Nothing -> let newval = y + (eta * (r - y))
>                                                                           in take (dirIndex a) sa ++ [newval] ++ drop ((dirIndex a) + 1) sa
>                                                  in Map.insert s qt q
>                                   Nothing -> let y = 0.0
>                                                  snext = Map.lookup s' q
>                                                  qt = case snext of
>                                                               Just z -> let newval = y + (eta * (r + (gamma * (maximum z)) - y))
>                                                                           in replicate (dirIndex a) 0.0 ++ [newval] ++ (drop ((dirIndex a) + 1) $ replicate 5 0.0)
>                                                               Nothing -> let newval = eta * r
>                                                                           in replicate (dirIndex a) 0.0 ++ [newval] ++ (drop ((dirIndex a) + 1) $ replicate 5 0.0)
>                                                  in Map.insert s qt q

We also need a quick way to generate the very first, zero-initialized Qmatrix, based on combinations over the
possible stateKeys. Note that there are some states generated by this algorithm that will never occur, such as
"wwwwo" (a cell surrounded by four walls), but the cost to memory and time is minimal, so we don't really care.

> newQTable :: Map String [Double]
> newQTable = let strings = [ [n] ++ [s] ++ [e] ++ [w] ++ [h] | n <- [E .. W], s <- [E .. W]
>                                                   , e <- [E .. W ], w <- [E .. W], h <- [Rc .. Re]]
>                 in Map.fromList $ map makePair $ map stateKey strings
>                     where makePair x = (x, [0.0,0.0,0.0,0.0,0.0])

A test step is fundamentally similar to a training step. Note the lack of a call to updateQ.

> test          :: Double -> (Qmatrix, Board, Double) -> IO (Qmatrix, Board, Double)
> test eps (q, b, r)   = do let s = observe b
>                           --showBoard b
>                           t <- isRandom eps
>                           --print ("Is action random: " ++ show t)
>                           a <- if t then action 5 else bestAction s q
>                           let (s', r') = move a b
>                           --print ("Action: " ++ show a)
>                           --print r'
>                           --throwaway <- getLine
>                           return (q, s', r'+r)

=============================
	\end{lstlisting}

	\begin{lstlisting}[language=Haskell,numbers=none,basicstyle=\tiny]
Qlearn.lhs
Cody Shepherd

> module Qlearn where

> import Learning
> import Board
> import Data.List
> import Control.Monad
> import System.Random
> import Data.Map (Map)
> import qualified Data.Map as Map

The key piece in this project that eluded my during my last attempt was 
being able to "loop" a given number of times, repeating a computation and
accumulating and updated model.

In the definition of this algorithm we define an episode of 200 training 
steps during which the Qmatrix model is updated and accumulated; at the end
of the episode the board is thrown out but the Qmatrix is kept for further
training.

The Kleisli composition operator is going to facilitate this for us.

First we need some constants in order to keep function parameters minimal.
Ultimately it would be nice to specify these at runtime from a terminal, but
that's a project for another day.

> n_steps = 200
> n_episodes = 5000


To facilitate the above function we need to be able to randomly generate a 
starting board

dims: dimensions of board (excluding wall border)
p: % of cells with cans at start

> randBoard         :: (Int, Int) -> Double -> IO Board
> randBoard dims p  = do    let n = floor $ fromIntegral (fst dims) * fromIntegral (snd dims) * p
>                           cs <- randCans dims n
>                           r <- randRob dims
>                           return (Board(dims, cs, r))

dims: dimensions of board (excluding wall border)
n: number of cans desired

> randCans          :: (Int, Int) -> Int -> IO [Can]
> randCans dims 0   = return []
> randCans dims n   = do   i <- randomRIO(0, (fst dims)-1)
>                          j <- randomRIO(0, (snd dims)-1)
>                          let c = Can(i, j)
>                          cs <- randCans dims (n-1)
>                          return $ nub (c:cs)

dims: dimensions of board (excluding wall border)

> randRob           :: (Int, Int) -> IO Rob
> randRob dims      = do    i <- randomRIO(0, (fst dims)-1)
>                           j <- randomRIO(0, (snd dims)-1)
>                           let r = Rob(i, j)
>                           return r

The episode function conducts a training episode. It uses the Kleisli
operator to pass the results of each training step from one step to 
the next, and come out at the end with the final accumulated result.

dims: dimensions of board (excluding wall border)
p: % of cells with cans at start
n: number of steps in episode
eps: epsilon value
q: zero-initialized Qmatrix
i: episode number
r: The rewards list from the previous call to episode, to be extended

> episode                         :: (Int, Int) -> Double -> Int -> Double -> (Qmatrix, Int, [Double]) -> IO (Qmatrix, Int, [Double])
> episode dims p n eps (q, i, r)  = do  b <- randBoard dims p
>                                       let eps' = if i `mod` 50 == 0 && eps > 0.1 then eps - 0.01 else eps
>                                       (q', b') <- foldr1 (>=>) (replicate n (train eps')) (q,b)
>                                       --print ("episode" ++ show i)
>                                       let r' = sum $ map sum (Map.elems q')
>                                       return (q', i+1, r':r)

doTraining conducts the entire training phase by folding the Kleisli 
operator over episodes.

dims: dimensions of board (excluding wall border)
p: % of cells with cans at start
eps: initial epsilon value, to be annealed

> doTraining            :: (Int, Int) -> Double -> Double -> IO (Qmatrix, [Double])
> doTraining dims p eps = do    (qfinal, i, rs) <- foldr1 (>=>) (replicate n_episodes (episode dims p n_steps eps)) (newQTable,1, [0])
>                               return (qfinal, reverse rs)


With training conquered, our next task is to conduct testing, wherein the robot makes a 
guess but does not update its Q matrix.

These functions work in the same way as the training functions.

dims: the board dimensions, as above 
p: the percentage of cans on the board 
n: the number of test episodes to run 
eps: the epsilon value (remains constant during testing)
q: the trained Q Table 
r: The rewards list from the last call to testEpisode, to be extended

> testEpisode                       :: (Int, Int) -> Double -> Int -> Double -> (Qmatrix, [Double]) -> IO (Qmatrix, [Double])
> testEpisode dims p n eps (q, r)   = do    b <- randBoard dims p
>                                           (q', b', r') <- foldr1 (>=>) (replicate n (test eps)) (q,b,0.0)
>                                           --print ("testEpisode" ++ show i)
>                                           return (q', (r':r))
>                                           

The parameters for this function are the same as above.

> doTesting             :: (Int, Int) -> Double -> Double -> Qmatrix -> IO (Qmatrix, [Double])
> doTesting dims p eps q  = do  (q', r) <- foldr1 (>=>) (replicate n_episodes (testEpisode dims p n_steps eps)) (q, [0.0])
>                               return (q', reverse r)

=============================
	\end{lstlisting}

	\begin{lstlisting}[language=Haskell,numbers=none,basicstyle=\tiny]
> module Main(main) where
> import Board
> import Learning
> import Qlearn
> import qualified Data.Map as Map
> import Data.List
> import Graphics.Rendering.Chart.Easy
> import Graphics.Rendering.Chart.Backend.Cairo


What I'd like to do here is set up my initial board, set up my zero-initialized Qmatrix, 
and then "iterate" train on those initial parameters, passing its results into the next
computation, for some number of times. I.e. I would like to "accumulate" the results of
each successive call to train.

I've written a couple of helper functions here to assist with plotting the results of 
the experiment as desired.

> zipInd    :: [Double] -> [(Int, Double)]
> zipInd rs = [0..] `zip` rs

> average xs = realToFrac (sum xs) / genericLength xs

> every n xs = case drop (n-1) xs of
>               (y:ys) -> y : every n ys
>               [] -> []

> main = do putStrLn ("Training...")
>           (qfinal, rwds) <- doTraining (8,8) 0.5 1.0
>           putStrLn ("Training finished.")
>           --print qfinal
>           putStrLn ("Testing...")
>           (q', rs) <- doTesting (8,8) 0.5 0.1 qfinal
>           putStrLn ("Testing finished.")
>           --print $ average rs
>           let tot = foldr (+) 0.0 rs
>               avg = tot / (fromIntegral $ length rs)
>               epis = every 100 rwds
>           toFile def "rewards_eps0_1.png" $ do
>               layout_title .= "Test Reward Avg" ++ (show avg)
>               layout_x_axis . laxis_title .= "Episode (x 100)"
>               layout_y_axis . laxis_title .= "Reward in Q Table"
>               plot (line "eps=0.1" [zipInd epis])
>               return ()

=============================
	\end{lstlisting}

	\section{Python Code}
	\begin{lstlisting}[language=Python,numbers=none,basicstyle=\tiny]
"""
Cody Shepherd
qlearn.py
CS 545: Machine Learning
Homework 6

INSTRUCTIONS ON RUNNING
In command-line: python qlearn.py
Or open in PyCharm and run qlearn.py

REQUIREMENTS
no external files are required for this program to run
"""

from enum import Enum
import random
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages


class Cell(Enum):
    """
    This class describes the state of a given cell or square in the grid environment where
    this AI will learn. Also included are some static methods for generating string representations
    of cells and generating random cells.

    As an enumerated type, objects of this class will have only one of the values declared below.

    "ERob" and "CRob" represent Rob, the AI, on Cells where a Can does not and does also reside,
    respectively.

    """

    Empty = 0
    Wall = 1
    Can = 2
    ERob = 3
    CRob = 4

    def __str__(self):
        """
        This function converts a given cell to the string required to place it in the pictoral
        representation of the grid space

        :return: three-character string

        """
        if self.value == 0:
            return "[ ]"
        elif self.value == 1:
            return " = "
        elif self.value == 2:
            return "[c]"
        elif self.value == 3:
            return "[o]"
        else:
            return "[8]"

    @staticmethod
    def toString(c):
        """
        This function converts an integer argument to the string value appropriate for
        placement in the grid space.

        :param c: int : 0 <= c < 5
        :return: three-character string

        """
        if c == 0:
            return "[ ]"
        elif c == 1:
            return " = "
        elif c == 2:
            return "[c]"
        elif c == 3:
            return "[o]"
        else:
            return "[8]"

    @staticmethod
    def canOrEmpty():
        """
        This function returns either an empty cell or a can cell, at random.

        :return: Cell

        """
        i = random.randint(0, 100)
        if i < 50:
            return Cell.Empty
        else:
            return Cell.Can


class Board:
    """
    This class represents the grid space / game board and the "pieces" on it. It maintains the
    location of the "Rob" figure, and determines rewards given to the AI for different actions.

    dirs :: [String] : static field enumerating the actions available to the AI
    tax :: Boolean : flag indicating whether an "action tax" is enabled (True) or not (False)
    base_penalty :: Float : value determining the base penalty (without tax) returned when the
                            AI attempts to pick up a can where there is no can
    grid :: [[Cell]] : a 2-d list of cells that comprises the grid space
    location :: (Int, Int) : a tuple representing the location of the AI. For (i, j), i represents
                            the "x-coordinate," i.e. the index within the nested list, and j
                            represents the "y-coordinate," i.e. which nested list to look for. This
                            means you will see Rob's location as grid[j][i].

    """

    dirs = ["up", "down", "right", "left", "pickup"]

    def __init__(self, dim=10, tax=False, rw=1.0):
        """
        Constructor.

        :param dim: Int : determines the height and width of the grid space, including walls
        :param tax: Boolean : whether or not an "action tax" (reward of -0.5 for taking any
                                action) is enabled
        :param rw: Float : the base penalty for an incorrect pickup action

        """
        self.tax=tax
        self.base_penalty = rw
        self.grid = []
        self.grid.append([Cell.Wall for i in range(dim)])
        for i in range(dim-2):
            self.grid.append([Cell.Wall] + [Cell.canOrEmpty() for j in range(dim-2)] + [Cell.Wall])
        self.grid.append([Cell.Wall for i in range(dim)])
        self.location = (random.randint(1, 8), random.randint(1, 8))
        self.addRob(self.location)

    def __str__(self):
        """
        String conversion method.

        :return: string representing grid space for pretty printing

        """
        st = ""
        for line in self.grid:
            for item in line:
                st += Cell.toString(item)
            st += '\n'
        return st

    @staticmethod
    def actToInt(act):
        """
        This method maps an action to its corresponding integer value, i.e. which column in the
        qtable defines this action.

        :param act: String : the action to be mapped
        :return: Int : the index of the action

        """
        if act not in Board.dirs:
            raise BaseException("actToInt given illegal action")

        if act == "up":
            return 0
        elif act == "down":
            return 1
        elif act == "right":
            return 2
        elif act == "left":
            return 3
        elif act == "pickup":
            return 4
        else:
            raise Exception("Illegal action in actToInt.")

    @staticmethod
    def stateToKey(state):
        """
        This method maps a list of Cells to a numeric string that will represent it in the
        qtable.

        :param state: [Cell] : a list of five Cells
        :return: String : numeric string representing state's "key"

        """
        if len(state) != 5:
            raise Exception("State passed to stateToKey is wrong length!")

        st = ""
        for cell in state:
            if cell == Cell.Empty:
                st += "0"
            elif cell == Cell.Wall:
                st += "1"
            elif cell == Cell.Can:
                st += "2"
            elif cell == Cell.ERob:
                st += "3"
            elif cell == Cell.CRob:
                st += "4"
            else:
                raise Exception("Unknown State encountered")
        return st

    def getState(self):
        """
        Returns the state of Rob's current location, i.e. what lives at north, south, east, west, and
        "here," in that order.

        A state is represented by a dict of Cells with keys 'n' 's' 'e' 'w' and 'h'.

        :return: {String:Cell} : a dict representing the state at self.location

        """
        i = self.location[0]
        j = self.location[1]
        lst = []
        state = {}
        state['n'] = self.grid[j-1][i]
        lst.append(state['n'])
        state['s'] = self.grid[j+1][i]
        lst.append(state['s'])
        state['e'] = self.grid[j][i+1]
        lst.append(state['e'])
        state['w'] = self.grid[j][i-1]
        lst.append(state['w'])
        state['h'] = self.grid[j][i]
        lst.append(state['h'])
        state['k'] = self.stateToKey(lst)
        return state

    def addRob(self, loc):
        """
        Adds Rob to the given location. This is an in-place update.

        Note that this method should probably only be called after calling self.removeRob(), otherwise
        there will be more than one Rob on the board.

        :param loc: (Int, Int) : tuple representing the location at which to add Rob
        :return: None

        """
        i = loc[0]
        j = loc[1]
        lc = self.grid[j][i]
        if lc == Cell.Empty:
            self.grid[j][i] = Cell.ERob
            self.location = loc
        elif lc == Cell.Can:
            self.grid[j][i] = Cell.CRob
            self.location = loc
        else:
            raise Exception("Trying to add Rob in a Wall")

    def removeRob(self, loc):
        """
        Removes Rob from the given location. This is an in-place update.

        Note that this method also voids self.location.

        :param loc: (Int, Int) : the location of Rob, from which xhe will be removed.
        :return: None

        """
        i = loc[0]
        j = loc[1]
        lc = self.grid[j][i]
        if lc == Cell.ERob:
            self.grid[j][i] = Cell.Empty
            self.location = None
        elif lc == Cell.CRob:
            self.grid[j][i] = Cell.Can
            self.location = None
        else:
            raise Exception("Trying to remove Rob and he's not there")

    def moveRob(self, dir):
        """
        Updates the grid to reflect the specified movement, and returns the reward for said
        movement. This is update-in-place.

        :param dir: String : one of self.dirs
        :return: Float : reward for action

        """
        if dir not in self.dirs:
            raise Exception("Illegal move")

        i = self.location[0]
        j = self.location[1]

        if dir == "up":
            nj = max(j-1, 1)
            self.removeRob(self.location)
            self.addRob((i, nj))
            if nj == j:
                return -5.0 if self.tax is False else -5.5
            else:
                return 0.0 if self.tax is False else -0.5
        elif dir == "down":
            nj = min(j+1, 8)
            self.removeRob(self.location)
            self.addRob((i, nj))
            if nj == j:
                return -5.0 if self.tax is False else -5.5
            else:
                return 0.0 if self.tax is False else -0.5
        elif dir == "left":
            ni = max(i-1, 1)
            self.removeRob(self.location)
            self.addRob((ni, j))
            if ni == i:
                return -5.0 if self.tax is False else -5.5
            else:
                return 0.0 if self.tax is False else -0.5
        elif dir == "right":
            ni = min(i+1, 8)
            self.removeRob(self.location)
            self.addRob((ni, j))
            if ni == i:
                return -5.0 if self.tax is False else -5.5
            else:
                return 0.0 if self.tax is False else -0.5
        elif dir == "pickup":
            if self.grid[j][i] == Cell.CRob:
                self.grid[j][i] = Cell.ERob
                return 10.0 if self.tax is False else 9.5
            elif self.grid[j][i] == Cell.ERob:
                return -self.base_penalty if self.tax is False else -(self.base_penalty+0.5)
            else:
                raise Exception("Trying to pickup and Rob is not there")


class Qtable:
    """
    This class stores and manages the q-table or q-matrix used in the q-learning algorithm
    directing Rob's AI.

    table :: {String : [Float]} : a dict implementing the Q-matrix. Keys represent the state
                                    label "column", and values represent the learned reward
                                    values for each possible action, indexed the same as
                                    the output of Board.actToInt()

    """

    def __init__(self):
        """
        Constructor. Generates all possible 'nsewh' strings, with 'nsew' over [0-2], and
        'h' over [0-4]. This is a bit overkill, as not all possible state combinations
        will be encountered, but it is easy to accomplish programmatically.

        It is also true that 'h' (here) does not need 5 states, only two, but I am going
        to overlook that for now.

        :return: None

        """
        strings = [chr(n) + chr(s) + chr(e) + chr(w) + chr(h) for
                   n in xrange(ord('0'), ord('3')) for
                   s in xrange(ord('0'), ord('3')) for
                   e in xrange(ord('0'), ord('3')) for
                   w in xrange(ord('0'), ord('3')) for
                   h in xrange(ord('0'), ord('5'))]
        self.table = {n: [0.0, 0.0, 0.0, 0.0, 0.0] for n in strings}

    def __str__(self):
        """
        String conversion method. This outputs the contents of the entire table, so beware,
        especially if the table dimensions are high.

        :return: None

        """
        st = ""
        for s in sorted(self.table.keys()):
            st += s + ' ' + str(self.table[s]) + '\n'
        return st

    def update(self, state, act, state_p, act_p, eta, gamma, reward):
        """
        Updates the q-matrix according to the q-function. This is an in-place update.

        :param state: Dict : dict representing state when action 'act' was taken
        :param act: Int : number representing action taken, corresponding to Board.actToInt()
        :param state_p: Dict : dict representing s', or the state after action 'act was taken.
        :param act_p: Int : number representing best possible action from s'
        :param eta: Float : hyperparameter of Learner, representing the learning rate
        :param gamma: Float : hyperparameter of Learner, representing the "discount"
        :param reward: Float : reward obtained from action 'act' in state 'state'
        :return: None

        """
        q_sa = self.table[state['k']][act]
        q_sap = self.table[state_p['k']][act_p]
        self.table[state['k']][act] = q_sa + eta*(reward + (gamma*q_sap) - q_sa)

    def sum(self):
        """
        Returns the sum of all values held in self.table.

        :return: Float

        """
        ls = np.array(self.table.values())
        s = np.sum(ls)
        return s


class Learner:
    """
    This class manages and executes the q-learning algorithm.

    init_eps :: Float : initial epsilon value, representing degree of randomness in actions taken
    t :: Boolean : whether or not action tax is enabled
    rw :: Float : base "reward" for incorrect pickups. Note that this value should be positive, even
                    though it will eventually be given a sign change.
    qt :: Qtable : the q-matrix used by this Learner
    board :: Board : the board used by this learner (will be reinitialized between episodes)
    step_rewards :: List : keeps track of rewards produced in each step. reinitialized often
    reward_sums :: List : keeps track of rewards produced every 50 episodes
    eps_dec :: Boolean : whether or not annealing is enabled for the epsilon value
    N :: Int : the number of episodes to run
    M :: Int : the number of steps per episode
    eta :: Float : the learning rate
    gamma :: Float : the discount factor
    epsilon :: Float : the "randomness" factor

    """

    def __init__(self, eps_dec=True, N=5000, M=200, eta=0.2, gamma=0.9, eps=1.0, tax=False, rw=1.0):
        """
        Constructor.

        See class docstring for descriptions of parameters.

        """
        self.init_eps = eps
        self.t = tax
        self.rw = rw
        self.qt = Qtable()
        self.board = Board(tax=self.t, rw=self.rw)
        self.step_rewards = []
        self.reward_sums = []
        self.eps_dec = eps_dec
        self.N = N
        self.M = M
        self.eta = eta
        self.gamma = gamma
        self.epsilon = eps

    def getRandomAction(self):
        """
        Returns a random action from Board.dirs

        :return: String

        """
        r = random.randint(0, 4)
        a = self.board.dirs[r]
        return a

    def getBestAction(self):
        """
        Returns the action with the highest value in the q-matrix row of the current state of
        the grid. If all values are the same, returns a random action.

        :return: String

        """
        k = self.board.getState()
        k = k['k']
        vals = self.qt.table[k]

        if all(vals) == vals[0]:
            return self.getRandomAction()

        i = int(np.argmax(vals))
        a = self.board.dirs[i]
        return a

    def newAction(self):
        """
        returns either a random action or the "best" action (which may be random), depending
        on the value of epsilon and RNG.

        :return: String

        """
        t = random.random()
        if t > self.epsilon:
            return self.getBestAction()
        else:
            return self.getRandomAction()

    def step(self):
        """
        This method represents one step of computation in an episode, or one "move" made by
        Rob's AI. This method follows the Q-learning algorithm, and does in-place updating
        of its data structures.

        :return: None

        """
        state = self.board.getState()
        act = self.newAction()
        a = self.board.actToInt(act)
        reward = self.board.moveRob(act)

        state_p = self.board.getState()
        act_p = self.getBestAction()
        a_p = self.board.actToInt(act_p)
        self.qt.update(state, a, state_p, a_p, self.eta, self.gamma, reward)

    def episode(self):
        """
        Executes M steps, or one episode, re-initializing self.Board beforehand.

        :return: None

        """
        self.board = Board(tax=self.t, rw=self.rw)
        for i in range(self.M):
            self.step()

    def train(self):
        """
        Executes training, comprised of N episodes of M steps, wherein the algorithm is permitted
        to update its q-matrix.

        This method also outputs some results in the form of a pdf and a text file.

        :return: None

        """
        for i in range(self.N):
            if i % 50 == 0 and self.epsilon > 0.1 and self.eps_dec is True:
                self.epsilon -= 0.01
            self.episode()
            self.reward_sums.append(self.qt.sum())
            # print str(self.board)
            # print "Total reward: ", self.reward_sums[-1]
            # print "End of Episode: ", i
        # print "Final total reward: ", self.reward_sums[-1]
        pp = PdfPages("PU_Pen_" + str(self.rw) + "_tax_" + str(self.t) + "_epsdec_" + str(self.eps_dec) +
                      "_Gamma_" + str(self.gamma) + "_Eta_" + str(self.eta) +
                      "_N_" + str(self.N) + "_M_" + str(self.M) + "_ieps_" + str(self.init_eps) + ".pdf")
        plt.figure()
        labels = [str(x) for x in range(0, self.N, 100)]
        parts = [self.reward_sums[i] for i in range(0, len(self.reward_sums), 100)]
        plt.plot(range(self.N), self.reward_sums)
        plt.plot(labels, parts)
        plt.xlabel("Episode")
        plt.ylabel("Sum of Reward in Q Table")
        plt.title("PU Pen: " + str(self.rw) + " Tax: " + str(self.t) + " Eps Dec: " + str(self.eps_dec) +
                  " G: " + str(self.gamma) + " Eta: " + str(self.eta) +
                  " N: " + str(self.N) + " M: " + str(self.M) + " i_eps: " + str(self.init_eps))

        pp.savefig()
        pp.close()
        plt.close()

    def test_step(self):
        """
        A step in testing, during which the algorithm is not updating its matrix.

        This method returns the reward it obtains from each move, for measurement purposes.

        :return: Float

        """
        act = self.newAction()
        reward = self.board.moveRob(act)
        # print self.board
        # raw_input()
        return reward

    def test(self):
        """
        A testing iteration, consisting of N testing episodes of M testing steps.

        This method tracks the rewards obtained at each step, and at each episode, and
        reports the average and standard deviation of rewards at each episode in a text file.

        :return: None

        """
        self.reward_sums = []
        self.epsilon = 0.1
        for i in range(self.N):
            self.step_rewards = []
            self.board = Board(tax=self.t, rw=self.rw)
            for j in range(self.M):
                self.step_rewards.append(self.test_step())
            self.reward_sums.append(np.sum(self.step_rewards))

        avg = np.mean(self.reward_sums)
        stdev = np.std(self.reward_sums)

        with open("PU_pen_" + str(self.rw) + "_tax_" + str(self.t) + "_epsdec_" + str(self.eps_dec) +
                  "_Gamma_" + str(self.gamma) + "_Eta_" + str(self.eta) +
                  "_N_" + str(self.N) + "_M_" + str(self.M) + "_Ieps_ " +
                  str(self.init_eps) + ".txt", 'w+') as fh:
            fh.write("Test Average: " + str(avg) + '\n')
            fh.write("Test Standard Dev: " + str(stdev) + '\n')

# Experiment 1
l = Learner()
l.train()
l.test()
	\end{lstlisting}

	
            
            
\end{document}